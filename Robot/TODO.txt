Critical
--------

With raspberry
- tensorflow Version: 1.14.0

- model coco v1 can get (got once)

2020-02-28 18:59:09.755980: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:573] memory_optimizer failed: Deadline exceeded: memory_optimizer exceeded deadline., time = 563687.375ms.
tensorflow/core/grappler/optimizers/meta_optimizer.cc:573]

and TensorflowClassification will stop



Develop
-------

Todo
----

- Make clearer git-directory order
-- 1) Wall-E
   Project-name
   - All stuff, introducing stuu etc, code, ...
--- 2)Robot
    - Robot platform
---- 3) linux
     - linux specific files like starting service   
---- 3) python3
----- 4) Robot
      - python3 Robot module implementation
----- 4) package
      - package making scripts
      - package     
---- 3) android
     - android implementation
     
Current                 To be moved

1) Wall-E               1) Wall-E OK
                           2) Robot new
                              3) python3 new
                                 4) Robot moved from Wall-E/Wall-E/Robot
                                 4) package includes moved from Wall-E/Wall-E/setup.py
   2) Wall-E
      3) Robot
                              3) linux moved from Wall-E/Wall-E/scripts  
      4) scripts
                              3) android new

       
 
- Android-version

- Study Presence by Location -consept
  Maybe we should implement also some location group where this kind action is valid.

- Add ReactionToSensation-attribute to Sensation so if we we have many Robots in the network, then first can one reacts
  in Communication and also in TensorflowClassification to create new Sensations based on source Sensation(s). Other Robots
  must not react when or to some actions to like play a sound if some other Robot has done it already.

- (Main)Robot should be a Sensation or should include sensation(s) that have identity information
  and associations to them , meaning that Robot can feel of things (Sensation)
- (Main)Robot should talk as it's kind. Try that wall-E -kind Robot speeks for
  instance 2/3 rate ( meaning, that we use produce 2/3 sample amount of 1
  
- Robot should deliver sensations only to subRobots running
-- low priority, because problen comes, if some subrobot die and they don't if no developing problems.  
  


What is done
------------

- Architecture, that is
-- devided into Robots that are implemented by Bobots, meaning
--- inheritance
---- Robot can be anywhere in one running instance
--- multihosted, Robots can be anywhere in the world, in any host
-- identity
--- Robot knows some details of itself
-- Virtual instances
--- this is for testing purposes
---- We can create sensations to our Real instance and study how it reacts
     meaning that we can finally start to implement thast our Robots gets Sensations
     that nake to fall in love, our final goal, make a Robot that can feel.
     But before that we have some to do.
-- Static LongTerm Memory
---- All whar Robot knows about this worlds in imlemented as Senmation class
    instances which are connected with each other. Anything other way is used.
    This momocs human memory. 
--- When a rub stoppes, we can save satus of memory and kload it back, when
    new run starts 
-- Item
--- This is a Sensation connected to other Sensation, just a name of a thing
    and a  score for this
-- Tensorflow classification to analyse Images Robot gets. This produces
   Items connected to subImages, cropped from camera Images is Classification
   results. This proces gives to Robot capability so see thing and give a name
   to the seen things and capability to connect other Sensations like voices
   to this Item. This mimics our brain main functionality: we are good at 
   processing visual things and that is most that our brains do. That will be
   main thing Robot will do, just process Images and think what it has seen.
   
What is not done and needs implementation
-----------------------------------------

Robot.process
- divide into two phasess
-- processSensation(sensation)
--- if there is something in Axon, process it
    AlsaPlayback is ecaple of implementation and
    also LoggerRobot explayned below
-- produceSensation()
--- when Sensation are processed, we can sense: as a sense produce a sensation
    default implementation for this is empty, but AlsaMicrophone is an example of implementation,
    as well raspberryPiCamera
- Sensation
-- Sourcerobot

- LoggerRobot
-- interrested of everything
-- Logges current sensations by order
-- whows images seen by order, yhese are diffent we show
-- echoes voices heard, these are different than voices spoken

- LoadSpeakerRobot == AlsaAudioPlayBack, for communicating by Voices with Items
-- TODO howto implement Robot which code is same than other,
   but work and /etc directory is different?
--- We can implement links, same way than linux does
--- or just inherit a robot by import,
    this gives us possibility to separater technology (AlseSpearker) and role (mouth, logger)
- ViewerRobot, for communication by imeges with Items
- We can name of main class of a seen thing. Names are like 'person',
  'dining table', 'bicycle', 'tv' etc., but we cant yet identity things inside
  main class like 'John' is a 'person'. So Robot don't know id it sees 'John' or
  'Joan', it sees just 'person'.
-- implementing this needs
--- live Tesorflow model. We use now foren one. Learning means that we should
    find out characteristic things between 'John' and 'Joan'. These can be voice
    or some statistical thing in images like colors.
- Live Tensorflow model for voices. Image gives a name of seen thing. We connect
  name to voice heard same time thing has been seen. Most potential way to
  implement this kind Ternsorflow learning model for voices is use converting
  scalar voice information as one channel (blachwhite) image information as
  describet in Tensorflow voice analysing totorial pages.
- Expectations.
-- We can study statistically what is happening in hours of day,
  days in week, etc. and make Robot to wait that same kind thins happen and if
  Robot finds out that thing are going on in normal way, make Robot happy. We
  are implementing Feelings. We are near out goal are'nt we?
-- When something happens like we hear a voice identified to Sensorflow to
   a class name, meaning on Item with this name, connected to other kind Sensation
   like Image, we can implement expectation: if something happens, then it
   is expected to happen also something else. Here we can make our Robot happy
   if Robot is right and it has leaned something of thos world. We are even closer
   to our goal, are'nt we?
   
Dependencies
------------  

dependencies: for pillow (pip3 install pillow)
sudo apt-get install libjpeg-dev -y
sudo apt-get install zlib1g-dev -y
sudo apt-get install libfreetype6-dev -y
sudo apt-get install liblcms1-dev -y
sudo apt-get install libopenjp2-7 -y
sudo apt-get install libtiff5 -y

sudo pip3 install pillow


# senses
sudo pip3 install pyalsaaudio
# needs to set AlsaAudioMicrophonw/etc/Robot.cfg
## default:CARD=Device

sudo pip3 install picamera
Camra must be enable from raspi-config

numpy needs
sudo apt-get install python-dev libatlas-base-dev

sudo pip3 install numpy

MainRobot needs
sudo pip3 install python-daemon
sudo pip3 install lockfile
sudo pip3 install psutil

(pip3 install pyyaml # Required to save tensorflow models in YAML format
 not yet implemented ready)

raspberry pi 3b+ Buster Lite
Needs pip3
sudo apt install python3-pip

Tensorflow with Lite, officional package is broken for Lite
python 3.5
https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp35-none-linux_armv7l.whl

python 3.7 (buster)
sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl
gives tensorflow==2.1, which is cuttent when this was written 
(if not work, old instruction
(https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp37-none-linux_armv7l.whl
tensorflow==1.8

Ubuntu 18.4
tensorflow==1.5? -> Last version to work with virtual machine
Ubuntu 14.04
sudo -H pip3 install --upgrade tensorflow --ignore-installed six
-- will get tensorflow 1.13
sudo -H pip3 install --upgrade tensorflow
-- will get tensorflow 1.14
requires
google.protobuf
matplotlib==2.1.0
requires
- sudo apt-get install libfreetype6-dev

OK also you need models from data, application should download these, but the is an error. TODO

Hardware
--------

Study these devices
- Sipeed MAix
-- Support MicroPython on M1
-- out of stock
- HUSKYLENS
-- kickstart,
-- emailed for supoort how to use and for free sample
- Google AI Camera set
- pixy2
-- color signature
-- no tensorflow
- Ai-Thinker ESP32-CAM
-- low
-- face recognition and detection

Develop
------
Done
----

OK Not needed any more
  starting MainRobot with Tensorflow by command line needs now
  PYTHONPATH=/home/reijo/models/research python3 MainRobot.py --start
  and you should copy git/Tensorflow/models/research to that place

OK Seems that capability sharing is not working: Wally does not know that PV can handle Voice In (voices to be spoken)
   Seems that capabilities are transferred OK

OK  We have setting and support for 1-channel microprone now
AlsaAudioMicrophone produces mono, 1-channel file with raspberry USB-microphone
even if setting are 2-channels. Still output is expected to have 2-channels.

OK  Check AlsaAudioMicrophonePlayback logic.
Should read microphone all the time, so we get right average voice level.
Should not be interesting if items are present.
If voice found, then should read as long as voice is on, so we get nice voice.

OK Found fast enoung LITE model and a way to run it for raspberry.
OK TODO check logic and test

- DONE permanent=False -attribute to sensations. This is not set as bytes, but is local property
OK Add not-saveable attribute isForgettable (int)/ reserve(),release() inUse(int) to Senation,
so these Sensation will be reachable as long as Robot.process will process them.
 
OK Start to use LongTerm-memory, when we get an response, so we remember things that should be rembembered.
In Memory-version is ready, but we should study also datavase version.

OK  File "/home/reijo/Wall-E/Robot/Sensation.py", line 1506, in removeAssociation
    del sensation.associations[i]
OK IndexError: list assignment index out of range
 
OK AlsaAudioPlayback: Normalize voices to play
OK Voice Kind cannot be local only, because Communinication and AlsaAudioBroadcast can be in different servers

OK Study memory usage
- import resource
- resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
Deprfecated Change Sensation.create so, that it returns an old Sensation if requested data is close enough.
            This would help also to memory problems, even id we still must implemnt some memory  management.
            Maybe not good idea. Or we should change old  Sensation with new one. But this is not simple to implement.

OK In raspberry application easily runs out of menory
OK -- should implement some method to remove more efficiently not so important sensations

OK def tracePresents(self, sensation):
- to Robot.method
- used in MainRobot
- OK We know present item-names now 

OK Robot.process to support asscociation
-- with that we can ass Feeling to processing (for instance speeking out with a feeling)
-- there is no implementation for this

OK Association to support presence

OK Develop VirtualRobot
- acts as Item.name
- produces Images as Camera does
- Produces Voices as AlsaMicroprone does
- Listens if Robot speaks and answers
- checks if is is marked to be present or not

OK Finalize presense
- Presence works in Tensorflow and AlsaMictophone, associating voices and Images to Item.name present

OK Comminication need to be implented also with supporting presense
- do this test drive
-- support testTensorlow. so real Image and Item sensations are created
-- Voices should be added manually, because AlsaAudioMicrophone can't be test.driven yelt, we can't emulate real devices

OK SocketServer keeps running on error
   SocketServer:('192.168.117.7', 2000):3:Interrupted: run: self.sock.recv(sensation_length) Interrupted error ('192.168.117.7', 2000) [Errno 9] Bad file descriptor


OK Make helper scrits to start and stop MainRobot and write to /tmp/files
   We have servoce-scrip now

OK SocketClient/SocketServer
  Check that it tells only pure Capabilities, not Capabilities from remote Robots
  Other way we get too much traffic. Architecture will be  1:n socket-view
   
OK Tensorflow
  Implement presense so that it report only presense changes
  Timestamp should be same than the original Image timestamp
  
OK Assosiation
  Keep status of present Item.names and assosiate only to present ones, not
  all the stuff.
  Time-based logic is not needed any more
  
OK Sensation
  Report hosts from sensatios is driven
  
 
OK AlsaAudiMicrophonePlayback does not stop. Add stop-handling


OK Implement manualThreading
- very light step that can be run, just sense() to queue or precess() from queue
  -- study how to do this
  -- priority of Robots
  -

OK Created 3 SocketServer and 3 SockertClient threads/instances. 1 +1 is enough.

