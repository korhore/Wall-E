Critical
--------
Too much logging as Normal
Visual, Socket*

  File "/home/reijo/git/Wall-E/Robot/python3/Robot/TensorFlowClassification/TensorFlowClassification.py", line 535, in run
    self.process(transferDirection=transferDirection, sensation=sensation)
  File "/home/reijo/git/Wall-E/Robot/python3/Robot/TensorFlowClassification/TensorFlowClassification.py", line 628, in process
    self.logAbsents(current_present=current_present)
  File "/home/reijo/git/Wall-E/Robot/python3/Robot/TensorFlowClassification/TensorFlowClassification.py", line 757, in logAbsents
    for name, presence in TensorFlowClassification.present.items():
RuntimeError: dictionary changed size during iteration
OOPS. there are two instaces of TensorFlowClassification running same time, normal and Identification. Ther can't be TensorFlowClassification.present,
but self.present
When Identifity communicates, does it get responses?

We shouls set limit for Communication responses per Concersation, 5 for instance to avoid loops and too much traffic.
Correct test

Forget proglem:
initial Ubuntu settings
maxrss = 2048
minavailmem = 4000
raspberry
maxrss = 768
minavailmem = 256

Seems that SocketClient has very many Sensations in its queue.
raspberry.SocketClient gets Sensation to its queue more often than
Ubuntu.SocketServer gets them.
This means that raspberry.SocketClient is too slow to send.

This was cause of whole Memory releasing problem. When Sensations are in Axon queue, they can't re released and thats why Memory releasing does nor work.
Put now max to Axon queue as well in SocketClient, but maybe SocketClient correction is not needed. We should consider, if Tennsorflow is hoo heavy and we should use
Lite only. At this point learnable Tensorflow benefits are not used so lite functionality is as good.

Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache for 2492 Total self.sensationMemory  usage 1347.3828125 MB available 4394.55859375 MB with Sensation.min_cache_memorability 1.5800000000000003
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache deletion skipped 1048 Not Forgottable Sensation

For some reason, there are Sensations, that will remain Not Forgottable
We must implement some way, how to keep track of these.

 359481 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache for 1859 Total self.sensationMemory  usage 719.25390625 MB available 10.5234375 MB
        with Sensation.min_cache_memorability 2.8799999999990162
 359482 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache deletion skipped 1014 Not Forgottable Sensation
 359483 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Eva number 10
 359484 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot MicrophonePlayback number 2
 359485 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Playback number 2
 359486 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Microphone number 2
 359487 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Visual number 2
 359488 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication number 939
 359489 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Wall-E number 164 ?? What is Wall-E, Identity
 359490 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Camera number 2
 359491 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot TCPServer: ('', 2000) number 2
 359492 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot SocketServer: ('192.168.117.130', 57016) number 5
 359493 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot SocketClient: ('192.168.117.130', 57016) number 2
 
 
raspberry
main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication SensationType Robot number 1
main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication SensationType Location number 1
main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication SensationType Item number 367
main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication SensationType Image number 198
main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication SensationType Voice number 64

Ubuntu
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache Not Forgottable robot Communication SensationType Robot number 1
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache Not Forgottable robot Communication SensationType Item number 39
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache Not Forgottable robot Communication SensationType Location number 1
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache Not Forgottable robot Communication SensationType Image number 13
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:Sensations cache Not Forgottable robot Communication SensationType Voice number 8


 main_Eva:Eva:1:Normal:raspberry livingroom:Sensations cache Not Forgottable robot Communication number 237
 - Still even if no attach and detached all
 -- This is logical, because weaded detach to sensation that are Azon put and that makes detach.
 -- Axon.get does not make detach(All), but maybe it should and after that attach(robot)
 
 attach/detach logic is broken and now for test reasons it does nothing with Axons, that change Forgattable to oine Robot no next one.
 Maybe we should add one meyhod more, just to do this ownership change, detach and attach in one method. Ataach when creating a Sensation is OK
 and it is OK that Robot.process we should detach Sensations Sensations when processing is done. It is OK to keep logic that we mark Sensations that are
 in active processing, not only in passive Memory.
 
 Another way to think think this case is that when processing is done, Sensations will change to Memorytype.LongTerm, but this way we will get too much Sensations,
 because there is Memory limit how many Senations we can keep.
 
 More important problem is that we wee lose available Memory somewhere. Robot keeps running in Raspberry no more than 12 h, but then
 Sensation.min_cache_memorability 24.420000000000076 is max and only 48 Sensations are kept.
 Seems that Memory handling works now fine untill we reach max of Sensation.min_cache_memorability. Sensation number in Memory can Variate a lot.
 In raspberry there can be at max 7000 Sensations witj 0.1 Memorbility, but then it can only 1000.
 Memory usage can of cource be much higher with SensationType.Image than SensationType.Item.
 Anyway we should test not tu use limit at all. and we should excees step or stop using associations with memorability calculation, because one Sensation
 where all are  associated, can make it impossible to release any otheers.

When MainRobot is very busy, Hurry, Busy, Breaking, we should make our subRobots slower. In practice we need sleep in Robot.Sense and Robot.process(). Maybe same mechanism as
in Memory handling.

Stop does not allways success.
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:logActivity activityAverage 0.2105919444658877 
Wall-E_MainName:father:1:Normal:Ubuntu livingroom:logActivity shortActivityAverage 0.14039462964392513 
These should stop if status is stop

Forget < 5 min neutral feeling and if it does not hel, forget feelings

Visual
When lof Communication, Feeling character on but Chabge empty, nothing else is on the line

Making 2 Robot in same location to communicate if they have same  sensation set
is no mean, because they communicate for ever. At least we should not forget used Robot voices
so 2 used voices array makes sense, 1 for person, 1 for robots.

Seems that Sensations make at least one trip back to origin host from other host.
Is the reason ReceivedFrom ''.'real receivedFrom'

OOPS clear configuration!

Check location/sublocation in configurations.
If in RaspBerryPiCamera config there are location localhost and under it 'location Ubuntu raspberry', then
two instances will be created. Ment to test 'sublocations'


Develop
-------

Implament 'all' in SensaTionType, MemopryType, RobotType
This is readonly, meaning that this defines what Robot acepts as capability.

How lown Identity runs. After along run, when stopping we get
Wall-E_MainName:father:1:Stopping:Ubuntu livingroom:MainRobot Stopping self.identity Identity
Wall-E_MainName:Identity:2:Stopping::Stopping robot

First implement Visual: posite/negative feedback in all parts of logic. with one method.

Seems wxPython is heavy to run, when there is load and many Sensatition density per time tyo handle.
Also Visual can be too hard to run. Anyway, seems it can be good idea  to implement Visual with kivy, because it
can be less heavy to run and it supports also Android to run same application as python.

State-Sensation for Communication, is this Robot is Waiting new Communication to start,
waiting new answer, meaning microphone is on or its is speaking, meaning that
it can't hear anything. And we need Implementation for it in Visual.

Make Conversation -> ConversationWithItem
- implementation from RobotType.Sense as it is
Make Conversation -> ConversationWithRobot
- needs planning
- We can have two Robot roles
-- 1) ComversationgRobot
      Robot that has conversation going on with a Item.name in a location
      and has MicroPhone and Playback-Robots in that location
-- 2) ConsultingRobot
      Robot that has some knowlehge what to say top Item.name in a conversation
      so that it responses, meaning that it just shares its best Voices and
      Images and this information should be based on real conversations.
      If we thinh to creatye a 'main knowledge', then role 1) gives feedback
      also to role 2) what was result spoked voices and images, meaning to
      send Feeling + their original Sensations, so Role 2) robot can collect
      this information basicaslly from many Robots, building assosiation map.
      Association map is that information what our Robots understand from the world.
- We need means to set those roles. Now we can use
-- DownLocations to those locations that Robots has MicroPhone and Playback-Robots and
   wants to be real Communicating Robot using role 1)
-- UpLocations to those locations that this Robot wants to be consulting Robots
  using Role 2)
  
Check Locations in Ubuntu ans Rasoberry. WE don't need Livingroom location any more,
but one mainRobot is responsible  of one location and other Robots can consult it.

Seems Robots presence is now local MainName presence, meaning local Main Robot, but should be opposite one,
 remore ones, got fromSocketServer and Local mainNames should be ignored.

Robot state is not clear enought for person.
1) Conversation on
2) Robot Waiting for answer,
3) Conversation eneen or Waiting someone to talk with
4) Conversation ended, because did not get answer
Anyway, we need Communication.Conversation.
Done/Doing

If Communication.Conversation is location based, then also feeling should be, instead of whole Robots Feeling.
Study Robots role in cobversation
(Robot1+person) + Robot2, means that Robots can conversate very fast with each other, but now person does not hear what other Robort says.
This is confusing. Robots1 answers to Robot2, but then person hears it. Robot2 can answer to person, and now person hears it. But when person answers,
does person answer to Robot1 or Robot2, Robots don't know it. Whole idea is a mesh. Maybe we should implement Rot-Robot conversation of a subject Item.name,
when person enters. And keep Conversation with person to be responsible only with that Robot, which is present in that location. And keep configurations so, that only one Robot conversates in one location with person. But Robot can still be present in many location to conversate.
downlocation/uplocations, robotlocations/communicationlocations for Robot tp Robot communication?

Robo1+Robot2 conversation of some subject

How to visualize it and implement new SensationType for it.

1. Minimize memory using
- test that we have reference to Sensation.image and Sensation.data, when
  we make a copy in all situations, also creating SWensation from binary
- Study Conversation
-- Conversation by Location
-- Conversation with Robots and persons, using person rules hearting and speaking.
   This is what is implemented now, but mixed. One logical Robot has Playback in Location and it
   speaks out TobotType.Muscle voices from itself and logical Robots present in this location.
   Robots speak RobotType.Copmmunication speak in their conversation, but person does not hear it, but in practice
   RobotType.muscle voices are same.
-- Conversation with Robots only, using Robots rules, where is no speaking and Hearing,
   but associations only

Now Sensation.dataId == Sensation.Id defines, if this Sensation is original
or copy from other Sensation. Thos wastes a lopt of Memory, because data is same and copied
and it does not change. More cleaver way to do ts same kind relationship as
Assiciation, one boolean field and getters will know where to find data.
Think again! Images take most of Memory. When we create a copy of Sensation,
we do 'destination.image = source.image' which means that in this phase no
PIL.image is replicated but python makes a reference to same data. Problem comes if
we make a binary file of original ans copy Sensations and create them.
Now we would get 2 different copies of PIL.image instances. Situation can be found in
load method  id != origibalId and corrected. Same situation is with voices.
We should study how many this king images there will be. It can be also so, that
original imageSensaton will not be loaded.
And if original imageSensation will be deleted, then copy ill be not copy any more,
but comes new original and it can be new original for many copies and only one
can be new original. Hmm. algorithm shouldf de studies how original with copies can be deleted.
Anyway, seems that original must know it copies.

2. Studu organisation Sensations by SensationType with
2.1 Dictionary of originals by SensationType
2.2 Dictionary of copies by SensationType

This can save Memory, because now there will be only one instance of data
with all SensationTypes of there used data.


picler fails
We should stop all MainRobot duties like logging activity ...
- open(fileName, wb) as file error cannot serialize '_io.TextIOWrapper' object
- picler is is replaced by simple binary  files, so we can forget this.

Robot can tell its capabilities to its parent. which creates it.
This prevents errors in configuration.
But configurability is still valuable feature. Can both fetures exist same time?
Maybe we could detect, if there is configuration mentioned in location,
whoch is usually 'localhost'.

Maybe we set capabilities first programmally and then override them with
capabiolities found from location section.

OK Remove Sensation.isCommunication
OK Add Sensation.RobotType.Communication 

Robot is also Item so it can be Preset in locations.

Memory.assign
- when we copy a sensation, assign it always to original one by default.

Communication
New new ideas
- Item.name is still special: we try to get infornation about Item.names, how to communicate with them.
-- If there is no conversation going on, we start a new conversation if any Sensation like these comes
--- Item.name comes present
--- if conversation delay is not going on
---- there is some item.names present ans we get any type Sensation
This means that we start a conversation always, whenthere is no consation going on and we have present Item.name

-- If there is conversation going on, we stop it if
--- last Item.name comes absent
--- we don't get any responses
---- we set conversation delay on if we have present Item.names. Thay don't a don't want to conversate

- when we start conversation we introduce ourserves
-- our item.name
-- Other current Sensationtypes like Image, Voice, Feeling. TODO What this means.

- When in conversation we get any type Sensation, we think it is an answer to us
-- we associate answer to
--- all present Intem.names
--- to out replys if there was some
-- we answer with
--- our Item.name This is for Robots
--- All type Sensation that we get as BestSensation associated tu one presenr Utem.name
    We will send Item.name as well
--- searching is done in order heanrd SensationType and best other Sensattypes assigned to it

Robot              Sense/Muscle                Human
             <-    Voice Sense Sensory
                   Voice Muscle Sensory ->
                   Image Muscle Sensory ->
                   
Seems that also rwo Robots can conversate each other
But then we need transforming Robot, that changes Sensations Muscle/Sense to opposite one.
Is?
- location: network
- SubRobot: Network
-- Is interested of all sensations.
-- Is this new Sensation.RobotType or Robot.RobotType?
                   
Robot              Sense/Muscle      Network   Sense/Muscle             Robot
             <-    Voice Sense Sensory  <-     Voice Muscle Sensory <-
             ->    Voice Muscle Sensory ->     Voice Sense Sensory  ->
             ->    Image Muscle Sensory ->     Image Sense Sensory  ->
             
Logical way to solve is to separate local and global (network) Robot functionality and/or
tranform. Where we mark local/network? Basi9cally Robot implementation should be local
to make things easy, so Sensation is not right place. Receiving Robot can be, but we must remeber
that transfer has now two fubctioalitis, share a sensation and conversate with sensations so
it seems that trnafer should have now one parameters mor share/conversate. Communication-Robot
should not know witj whom its is conversating, it implements local implementation only. We know
Sensations origin Robot site. And it is empty if local Sensation. If transferred from other site, it is not empty.
But Sensation can be shared/conversation. If we add Sensation.Function or Sensation.MenoryType.RobotCommunicate, we
can solve this.

Solution found. (MainNames) MainRobot.Name() is border. Robot belongs to some MainNames and if
Sensation is routes to other MainNames, then Muscle/Sensory is changed each other!
In pactice thios is done in SocketServer, which accepts Sensation.
Still there is problem to solve: What happens if Sensation is transferred to third Robot?
Hmm.., should we use arrays of locations? And what is role of Locations and  site addresseses?
Another is logical and another is technical, buy are that same thing? Well no. We can't share memory
between sites even if we try our best.

Seems we need also 'RobotCommunication' flag in sensation to mark, when Communication
has created this Sensation for communication.

To finalize this idea. we must
- terminology change who -> name
- check role of locations, name
- implement
-- nainName, mainRobots name
--- Config.py
--- Robot.py
                   
                   
                            
New ideas
- put incoming Sensationon in a queue or keep them in Axon
- Conversation keeps on, while we have incomins Senations
- For each Sensation, seach first best associated Sensation of any type,
for Item.Name we have present and then other type Sensations for that Sensation
associated for that one. Or also that type also.
- We don't stop conversation if we don't find any resposes for one Sensation, if we have
  other incoming Sensations, but keep on conversation
- Maybe this is common chaotic way to conversate for human beings.
  
Old ideas
- communication per item.name per location
- item.name can be got by Tensorflofclasification or by network
  representing present item.name speaking
- we can speak with voices, imageses or other types like other item.names and
- these all are assigned to this original item.name which represents this conversation
- conversation starts at all times with opposite item.name which is present and dont have any assignatuion yet
  and is Muscle-type, send to opposite (Visual)
- We try to respond with any type sensations once? But if we get answer we will interrupt this and answer to the last one?
  This makes conversation chaotic, but is it so in real life?
  -  There can be many conversation going on at same time, so response is prosessed in all conversations
     but still used voices/inages/items should be common.
- With Robots, we can communicate with any type Sensation, replly and answer one per one,
  but with Ronot - person, only voices are expected unti we give Vosual some tools to answer also with Images ans Items Robot knows.
  Item.names can be from Tensorflow-model, because Robot does not understand others, except if Robot names are also Iten.names.
  Images can be seen images, but only associated to Item.name, which is a problem or best scored images. Maybe last ones.   
  

- when we run out of bestSensations in speaking, what we will do?
-- stop answering ans wait conversation will end?
-- technically end conversation and start we one,
   meaning stat innredSensations can be ignored?
-- I will choose keeping conversation on. If we dond find anything to say
   ignoring what we hjave said before in this conversation, then we will say same things again or say again
   what opposite has said in this conversation. Maybe most interesting is say first what other side has said
   and after that say again what we save said, so we cound first add some parameters more into
   Memory.getMostImportantCommunicationSensations
   

ToDo Order
- remove
-- Robot.getLocations()
- enable
-- Robot.getDownLocation
-- Robot.getUpLocations

- precence by location
-- still to do
--- test first local Robot without tcp
--- then tcp

- Study Communication like speaking. When Item is present, we speak with
Playback device, but if Robot/VirtualRobot is present, we just send Sensation to it.
This means, that with Item.Name we connect a Communication way ro it.
-- with Robot.name, dont't connect any Muscles to speaking
-- with Item.came connect AudioPlayback to Voice
-- When presense is detected by some Robot. it also adds Routing to Muscles it support. This means that
--- Item is Robot, Should we add RobotType.Item
---- TensorflowClassificaton will do the job
----- Adds (Main)Robot
---(Main)Robot adds 'build' in routing for Robot, that don't have muscle configuration,
   because the at bond net
---- adds VirtualRobot
--- TCPServer adds network Robot
So what is missing to configure, is presence local, meaning that we use local muscles to deliver Voice to Item.name or
should we just deliver Voice to the Ribot. But we  basically always deliver to some Root. Basic questio still is, that
when presence is detected, we should configure how to deliver Muscle sensations to those Muscle Robots. So routing is missing one
parameter, targetRobot. It has now locations. TARGET can be sent by association. This as solution for communicarting with (Main)Robots
and with Item.names in other site.
-- add dataId to resolve that Sensation unique of Sensation (Voice, Image) data can be tectected, so same sata is not used in one conversation


- Create new Robot Projector
-- sceleton from Visual.Communication, but one 'line'
--- tab per location
-- Muscle side from Camera
-- takes input from Communucation
--- Communication 'speaks' now with images also
---- bestImage
---- waits some time answer as now so we show conversation state
     as we now do. Study id best way is extend Communication implementation
     or should we extend Robot.Control


- Create Robot.Control thread with AxonControl
  as control chanlel opposite to normal Axon data channel
-- states
--- not started
--- running
--- idle
--- ended
--- muscles should wait until their device has done thee job, like speaker
    so we can implement simple removing echo from microphone, if device does not do it
-- control
--- thread run and stop
---- start
---- end
--- Robot function controlled by semaphore
---- idle
---- running

- git pages similar as tensorflow how to install

To make a relase with installation package we need
- To make some soluion with Windows Visual implementation with wx
  -- There is throughput problem. It consimes a of resources and is really slow to update tabs.
  -- last wxPython-package does not work, so we must use some earlier one and now we try
  -- write installation pages like Tensorflow
  -- support Windows, Linux and raspberry with Camera. 
  -- single Robot configiration with Wall-E as main Robot and Eva with virtual Robot is fine
  
Study how to route item.presense. We should process image.sense in Tensorfolwclassification in 'in' location, where our own Camera is,
but we should produce item.presense to 'out' location, but we have not only one location direction. Robot.Sense produces something in int location, it is clear.
And Robot.muscle take in someting in its location, this is also clear. But Robos that handle or produce Sensation.Memorytype.Work, that is not clear.
Should they have own location consept, ay least out location. Or to be clear, should we extens location -> in_locations + out_locations.
I choose locations + in_location + out_locations. I in configutation there is onlo locations, we use it  in both way,
but if there are in_locations and out_location, then we we use that configuration. That way we take in onlu local camera input, but can produce
item.presence information to other hosts orking nea each other.

Maybe we should also use prence information handled by location, not global one. There is still study how to conversate. Only one Robot should coversate in one location
or atleast now many Robots should talk same time and if that has solution, than we should have solution how we let other Robot to know,
what other Robot said to person and not to listen. Something to study!

OK Combine RaspberrPiCamera and OpenCVCamera

Add voice changing algorith, so voices Robot speks will not be so clearly recognised same as  original ones, even if spek speed is changed.
Try these ideas
- NOK Changing a little bit the amplitude of each spectral component.
-- NOK We could add ramdon +- value for all values
  This adds only noise, but does not change voice

- NOK Multiply each frequency component not by k, but k+v(f), where v(f) shows variation dependent on frequency f.
  Such variation must be of much smaller value than k. I mean k >> v(f).
-- NOK We could all random +- value to iterator we use, when changing voice speed
 This adds only noise, but does not change voice
 
- Use vocoder idea
-- calculate frequency by time
-- regenetate voice randomly frequency indide these bands and voice should souns robot-like speech
-- we could also switch bands, fron highest to loowest and visda verse and chank, how it sounds
-- problem in frequency. It is easy to process amplitude, because it is sample item, bit frquency is high or loq maximun distance.
   Algorithm will be more chanllenging. WE need components like.
--- Calculate frequency of sample of some tine, 0.1 for instance
--- Regenerate voice of some frquency.
--- The voiced speech of a typical adult male will have a fundamental frequency from 85 to 180 Hz,
    and that of a typical adult female from 165 to 255 Hz.   

OK Combine Voice Robots, Alsa* and SoundDevice* so, that Robot uses either library it finds first. Technique is same thhan with Tensorflow.
SoundDevice Microphone and Playback don't seem to be independent. Check at least device settings.
Anyway, it can be impossible to use AlsaAudio for microphone and SoundDevicede as playback. And this is not use case.



Also add sine wave to voice, meaning that Robot is speaking.  

Implement 'global' location
- Robot has property locations, but normal case is that is has only one location
-- If Robot has in its config many locatiuons, MainRobot (in proctice, but also lower level Robots can have this configuration)
  created Robotinstance per location. So Robot has one location or in other words, Robot works in one location
- If Robot has 'global' in its locations, it will accept Sensations of all locations. This 'global' Robot creates subinstance of itself
  for all locations it will receive. Again, Robot works in one location.
-- With this feature we can implement 'centar kwnowlegde', familiar from many scifi films: dedicated Commnuication Robot in its special site
   handles Commumnication in all locations in the world,  handle and know what all perseon say in the world and learn how to communicate.
- Config has now "Default" section. In it "locations" defines configuration sections valid in those locations. Default values are still found
  from "Default" section.  location section does not contain "locations" attribute, because information is equal as section name and 
  other setting should not be used.
-- Configuration:
--- Now these ideas are implemented in Config.py and in all configutation files *.cfg so that
    Normal Robot location is found in Robot.config
    --- Section 'localhost'
    --- parameter 'sublocations'
    This is error in implementation and we shoul switch it 'locations'. 
-- DEFAULT_SECTION = 'DEFAULT' unchanged
-- DEFAULT_LOCATION = 'localhost' for compatibility
-- all Robot instancesof locations are running same time ans Config-file is open, to those Robot in stances should share same Congig-instance
--- add thread-safe
--- add location-parameter to all methods Robot-classes use.
    if place-section does not exist, we should create one. 
- Robot
-- __init__ should have more paremeter
--- location
    all running Robot has only one location, but it can be 'global'
--- config
    use 2 locations settings in localhost section
    - locations
    -- used as it has been now implemented, Robot can have many locations.
    - sublocations
    -- now Robot creates subRobots under it
    ---- subRobots has only one location, one of itsd parents sublocations
    -- its config comes from this order
       - locations[0] section
       - localhost section
       - default section
    instance of config
      If this is None
      - Robot creates its config in old way
      - Robot reads locations by config and creates subinstances by location of
        itself. this way it will be only one level more in Robot hierachly. This hierachly is not needed,
        so we could also create subrobots directly into MainRobot, This willl be more tricky.
        If we will create 'global' location, then this Robot will create on demand new subrobots, when
        it finds nedw location is nas not yer found, so one level more is acceptable solution.
        If there is only one location, the no subinstances are needed
      - If
-- Has MainRobot location(s)?
--- MainRobot gets its locations from subRobots, so no
    setLocation should do nothing.
    Robot finds its settings, when it uses its 'location' in all config-methods
createSensation
- parameter locations -> location
  because Robot worls in one single location and don't know other
Robot self.mainRobot.setLocations HOW?



Has Robot one or many locations? If many, are capabiolities different in these locations.
I choose
- many locations
-- locations in property of
--- Robot
--- Sensation
--- Config
but NOT
--- Capabilities
- Roots has same capabilities in all it's locations to keep things simple.
- still global location is possible, but this tme name is not '' but 'global'
- empty location '' is default, but it is always local, but it is global in local
  but is not sent to Remote/Virtual/Identity Robots.

--- Sensation
-- parameter/allribute location -> locations

--- Robot
--- Config


Back to basics.
- Make Sensation immutable
- remove all set-methods
-- setTime
--- used in test methods
-- setId
--- used once in test method
-- setRobotId
--- used once in Robot.py
-- setSensationType
--- not used
-- setRobotType
--- used once in Communication.py
-- setWho
--- not used
-- setLocation
--- used in test methods
-- setLeftPower
--- not used
-- setRightPower
--- not used
-- setHearDirection
--- not used
-- setAzimuth
--- not used
-- setX
--- not used
-- setY
--- not used
-- setZ
--- not used
-- setRadius
--- not used
-- setObservationDirection
--- not used
-- setObservationDistance
--- not used
-- setData
--- not used
-- setImage
--- not used
-- setFilePath
--- used 2 times in Sensation.py
-- setCapabilities
--- not used
-- setName
--- idef in tests
-- setScore
--- not used
-- setPresence
--- used in tests
-- setKind
--- not used
-- setFirstAssociateSensation
--- not used
-- setOtherAssociateSensation
--- not used
-- setFeeling
--- not used
-- setPositiveFeeling
--- not used
-- setNegativeFeeling
--- not used
- When creating Sensation, first copy propertieds from sensation-parameter
  and then overwite properties, that are not default ones.
- All are Sensations
- Make Robot a SensationType and Sensation
-- Robot class itself can't be Sensation, because it is nutable and Thread class denies mutability.
   Only solution is aggregate selfSensation and proviice functionality by that.
   We can implement association and use one Sensation to keep all Sensation information.
-- SensationType
--- Keep Configuration Who
--- Who -> Robot Who -> Robot
--- who -> robot
--- WHO -> ROBOT
- Make Capabilities Sensation
- With those you can assign
  -- Robot to 1 Capabilities
  -- Robot to many location
  --- Robot know its Locations its capabilities are valid
  --- Robot know its subrobots capabilities, becausu they can tell it. Routing can be done
  --- LATER We could study Star connected Robots, not only Up and Down, Parent - Children.
  -- Put in SensationType.Robot
  --- name
  --- Capabilities
  --- location
-- NO NO Assosiation is simple inner class of Sensation. i,plementing associations between Sensations.
   It includes properties of associations and there is now only one, Feeling and this is the way to
   present what Robot feel of this association. It is fine.  
-- We should also repalce Association with SensationType.Feeling. This is association, but
   relationship is 1:n, not 1:1, but at first SensationType.Feeling should work fine.
   Make test for this. First Sensation.associate should create SensationType.Feeling-type Sensation
   to assiciations-array.
  
- Study Location(s) if is a Sensation, not a special properity. Robot own it.
It can have name as not, but also coordinates.
We can decide, if Robot has enough knowledge to route Sensations down by location and Capabilities as now.
or if Robot decides itself what to do with a Sensation. Sensations are assigned to Location Sensation.
Location has
- X=0.0, Y=0.0, Z=0.0, radius=0.0   # location of Robot
- name
Implementation from Accelaration + name
Firsts step is implement Location-methods with SensationType.Location.

Capabilities should not contain locations,
but Config should have Locations -> Location


Study directory strucrure

Some ideas. Problem is loadModule, which is hard to change.
Base directory Robot
Platforn files
- Robot/android
- Robot/linux
- Robot/windows
pythob3-code in one level
Robot/python3.
- study if this works
Robot config files
- Robot/etc
Robot Identitys files
- per directory
-- Robot/Identitys/"name"
data
- Robot/data

Python base classes
Robot/Subrobot
- subsubots like Commnication etc.
Robot/etc
- config tiles for those, but also
  per config who as
  - *name.cfg
Robot/data
- data file for all
Robot/tests
If per file type
data -name data 1-level
etc - Robot.etc files
etc/InstanceName.cfg - Robot instance nale li Robot, Communication etc

Some Basics
- SensationType.Feeling is assosiate, so Robot.process(association... and
  Axon.put/get(Associsation... are not needed
- MainRobot has selfSensagtions, so SensaTionType.Feeling should aleays have firstSensation.

raspberry gets Ubuntu voices and visa versa, but why? They should not be interestedof other voices.
Hmm...
SocketClient: ('192.168.117.247', 2000):3:Normal:: got sensation from queue D Sun Jun 21 02:30:31 2020:1592695635.3461475:1592695852.6954086:Sensory:Sense:Voice:raspberry::Neutral:Normal
Seems that somehow location ib SocketClient/SocketServer is changed to be wrong. SocketClient should use location SocketServer gives.
TCPServer.connectToHost
- TCPServer.createSocketServer
- TCPServer.createSocketClient
- TCPServer.createSocketServer.start
--  if sensation.getSensationType() == Sensation.SensationType.Capability:
        # standard process for capabilities, this assembles our remote host capabilities and locations.
        self.process(transferDirection=Sensation.TransferDirection.Up, sensation=sensation)
        if self.getSocketClient() is not None:
                self.getSocketClient().shareSensations(self.getCapabilities())
                
We must set receivedFrom to out own sensation to reject unfished loop fot sending.
If we remove this, UI is OK, but functionality not.

Some Robot also generate sensation without locations.
Sommunication is problematic, because if we publish update to other sites, they
can't know that only memory should be updated. Without location this will be delivered
to all sites, is it valid in all sites in other locations, hmm...?


Implement exposure.

Separate (again) identiyu and exposures-
Make exposure a Sensation, so we can reconfigure Robot in live-

Implement exposure with base identity and all exposures per each with a fealing.

Better idea is to Memory.setMemorySensarion(feeling, ItemSensations,ImageSensations,VoiceSensations, feeling)
whics adds randomly associations between those. Problem is still that we need IdentityRobot to get ItemSensation,
ImageSensations, VoiceSesations and, which needs TensoflowClassification to that.
Lets implement and start with Identity, which will handle the job. 

Old idea is Exposure is VirtualRobot, but it uses MainRobots Memory
ins it does not have any other SubRobot than Identity. Reason is that
it will react to everyting just same way than MainRobot even it had Communication,
but it's meaning is exposure MainRobot to Memories, identity of Exposures identity,
what ity has looked like and what it has said. What is odd, is that it MainRobot speaks out
what it  responses to the Virtual Robot. Well anyway, we can do this exposure at backgrond
if we don't let any Sensation soutside Exposure Robot and don't let any Sensation to fo in
Exposure, letting it to live in its buddle, Try this nesx. We naed one Robot type more.

Should prevent Exposure to get any Sensations except Stop.
Styudy hasSubCapability and hasCapability
Exposure Communicates very long sequences, because it gets its responces back
but it should revers Muscle to Sensory.
Make test to test Exposure.

Identification. When identification introduces itself, it sens Sensations also
to Exposure, which responseses. Exposures Location is Exposure name, because
it is basically a memory.

Exposure in Indentity does not get Sensations. But is this right place for Exposure?
Try same level as Virtual
VIRTUAL and EXPOSURE Functionality is SAME. Check and remove Exposure.

When we run out of memory, MainRobot keeps to have Non Forgottable Sensation
It can mean that some Subrobot does not accept Sensation?
Increase. logging to Memory.
We can also just ignore NotForgettable, id we have tryed to get more memory some time,
but this decreas motivation to correct real problem with memory. But MainRobot
is not interested of Sensations, so it can Forget them anyway and tries.

More logging to non forgettables
When creating new SocketClient, trabsfer sensation fron old one to the new one.
This can be reason why we run out of memory.


Communication
Now we communicate with voices, but there is no reason, we we don't communicate with Items and Images also,
all kind of Sebsory type Sensations we have. We can coversate also of Fealings.
This means that we should repomd to all kind RobotType.Sensory with RobotType.Muscle Sensation.
Study if we could use palette in SensationType.

OK Robot Feeling
- implement Sensation.SensationType.Feeling with no associated sensation. Conversion to/from bytes don't work
  Make also feeling as a property of all SensationTypes

OK Robot Activity
- implement SensationType
- What we will do, when we Dream or Sleep?
  We should save our memory to ne ready for restart.
  Now this means that we delete all Sensory ans Work Sensations, but it does not matter,
  if we are sleaping. We should also restart our subrobots, so our Robot is reborn
  but all Memories are found, but Identity should not be started at start,
  keeps its state. Basically all other Senses ans Muscles have same situation,
  because they remember their state, even if onle AlsaAudioMicroPhone has something to remember.
  

To implement proper pickle, we need Sensation.__init__ with making a copy
without anything else, than LongTerm dependency.

father:1:Normal:Ubuntu: Sensations cache deletion skipped 30 Not Forgottable Sensation
25 father

Still need a proper Communication-tab in Visual

Sensation cache length variates in raspberry from ~200 to ~1000 Sensations,
but Robot will stay alive > 12h
but finally we get not forgettables and Memory will end.

Study TensorFlowClassification and do cleanup.

OK Should correct Communication to check input Sensation type and
not trust that configuration os OK.
Check Communication test.

OK Implement prevalence, meaning that we calculate number of connection and ages or
then for Item.name. That way we can guess, which Item.name is speaking.
Formula is += 1/Age() per every connection of Sensation.SensationType
self.prevalence={}                      # item.name prevalence, not variable, but function


Try centralized knowledle, tyhat knows everyting with communication.
This need communication with Communications per Location. 

We should share LongTerm-Sensations with Sites and not cre og Location in sharing
LonfTerm-Sensation to implement swarm intelligense.

Allow tcp-connection starting both sides, so last started site can connect
 
non-verbal expression
- is it enoungh to use Feeling sensation type

Alertness:
- sleeping, normal, tired etc.
-- this allows us imlement sleeping functionalitys like organice momory (save to database,
   pic longTerm SEnsations, dream (meaning what if somethong happenes or granice memory to more clear)

Allow non-verbal expression sense by keyboard
Implement non-verbal expression to Visual

Study swarm intelligense. Should we delegate communication as a best response to our swarm
and can we just choose best response. Can we define swarm intually? Or should we deny swarm
to communicate and reserve only one Communication per location. This means that Communication uses
that Memory, that is is found in its (computer's) MainRobot.

Do we try to reconnect to hosts, if connection is lost.

Next to do is is implement Location based Presence.
Also Communication should support Location(s). Can we communicate to 'other'
or many Locations. Yes, there is not only our own Location and in Memory, there
are Sensations from many Locations.
- But is this wice? Many communications in one place, where Robot should know that voice comes from other Robot,
  so itemName should be that Robot speking. But we don't handle speaking Item.name now very well,
  because we don't know what prent Irem.name is actually speaking. We should solve also this problem. 
  One solutions is mark in Micxrophone all present Item.names as speaker (just assign)
  and when speaker is Robot itself, set just Iten.Name==Robot.who as speaker.

Next Step would be to implement Awareness-type Robot. This is "new MainRobot
with new name", but it will include missing Awareness-features, when
technical MainRobot-features are moved to Robot-common implementation.
Awareness includes:
- Memory
- Feeling: mood of this whore Robot
- Activitylevel
- Location

Less Logging to Visual
Don't associate Sensory.Feeling in Memory, but forget when done

OK Next step will be study and implement Association in Axon, Robot.process in memory level Robots, meaning MainRobot level. Association has score and feeling, so syudy how to implemt 'feel better,
or feel worse about fron subject to object sensation.
- score Sensation.Assosiation ->  Sentation.Item feature
OK But Needs cleanup
OK Cleanup
Still change this as Feeling Sensationtype to be clear. This is always Feeling beetween Sensations
Assosiation as SensationType
- Assisiations handling has beeb there, but no as SensationType.
- Actuaslly we need two parameters, thw Sensations to be associated,
  This type sensation must always be Sensory MemoryType

Delete not forgettable sensation before pickle, because we can't pickle them because they are associated with Robot. Anyway, thay are too old or under process, so they are obsolete.

Virtual Robots Visual does not start always.
X-client Probots Visual does not start very quicly always.

It is not good idea to change Sensation processing Robot. It should be what it is.


Identity is hard to stop. How to awake from sleep.

When communicating, return also bestImage for Item.

Maybe Identiyy should be a Ropot thats runs once and then stops.
That way we could stop Main Robot
or maybe we could brutally kill Identity-process, that is sleeping by thread methods.

Show present items in Visual
identity sensations as Sensory MemoryType
tellIdentity for Robot functionality and VirtualRobot is only Role, not class
(tell)Identity as SubRobot, so it can wait without doing harm to Visual,
when main thread does not sleep?

Robot should have selfSensation of type item ans name Robots name.
With that we can describe Robot, what it has been seen and heard (out)
and what it has said and shown (what it looks like to others). A satrtsup it shares this
information to others.


Logical MainRobot should have only one Communication in its congiguration.
So when Logical MainRobot has many sites in its configuration its must not have many Communications
Virtual Robot should have one Communication, TensoflowClassification abs Visual and
it should borrow MicroPhone, Speaker, Cameresources from other. How this borrowing is done?
Same question is with MainRobot that does not have those resources of its own.
Now we have definition of MainRobot to down in one site, Maybe we should also define
configuration of borrowed up direction fron network or for virtual Robot direct in one site.


Forget Virtual Robot Sensations
Forget Robot Sensations
Valler Visual does noy stop

wx does not work well with another MainFrame with another Visual
[xcb] Unknown request in queue while dequeuing

(Robot.py:9815): Gdk-WARNING **: Robot.py: Fatal IO error 11 (Resource temporarily unavailable) on X server :0.

[xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
[xcb] Aborting, sorry about that.


Visial can be heavy. When stating many of them, delay and use Axon between Rpbot and wx-processes
to transfer Sensation and just awake wx with wx-metdod.

OK Wall-E:1:Starting: pickle.load(f) Exception Ran out of input

Associatian can be removed, because Memory has functionality as build in.

OK Implement Robot.isRunning also or use thread method. We nead that in many places.
Not needed in many places, only when we wait, that processes are realy stopped.


Fix SocketClient NotForgottables. It still can't forget in some situations
Move Robot-Association functionality tp Memory.
Memory is a place, that otganises this kind things.

OK Check Alsa-Robots.
Check if we echo Sensation to original source.

Organize Robots by MainRobot groups

Principles from lowe level Robot to higher ones

Each Sense-type Robot can belong to one or more Groups. It can be sensible, if
MainRobot and VirtualRobot share same Senses and Mucles. VirtualRobot does not have
it own lower level Seses or Muscles. This saring should be done by MainRobot
which is a brigde to Real Senses and Muscles of Virtual Robot, so VirtualRobot knows only its MainRobot
which has capabilities. For VirtualRobot this functiolality is already implemented.

MainRobot is ownwer of its Senses as Mucles as a Group. MainRobot can also be a member of group.
That way Group must have only one MainRobot, that has Memory, but all MainRobots group members
shjare this Memory. This way group has one shared Memory and MainRobot that has created that Memory
is a brighe to all subrobots.



Each Robot should belong to one exact Robot group, that has one MainRobot.
MainRobot must have a name, so groupsa name is MainRobots name.
Robot can be started in one site or other site. Other site has also MainRobot but it should
be handled as bridge/Nomrmal robot. Study if we can implement MainRobot as Robot,
so we could implement all this by one Robot class. Earlier problem was TVPServer, SocketServer,
Socketclient implementation. If that can't be done easily, rename MainRobot as BridgeRobot
and still start Robot from Robot-class, not from MainRobot class. AnyWay, that way Robot gets its role from
cconfifurayion ans Level. Level 1 Robots can/should have bridge(s).


Add Robot roles
- Remote -> RemoteSubRobot is part of remote MainRobot, meaning that it has only senses/mucle Robot
  that report to tja main Robot or get Sensation to mucles from remote Mainrobot
  even though this Sites MainRobot has MainRobot instance, that instance get its identity from the remote MainRobot
  but don't read any identity data from its directories.
- RemoteMainRobot This mainRobot think that other remore is independent MainRobot.
  Remote MainRobot has ts own identity, so it speaks as its own and shows images as its own.
  Two MainRobots cam coversate each with other and even be in love with each other.
  So we can make Wall-E and Eve-robots to be in love with each other.

- VirtualMainRobot is just like RemoteMainRobot but it is not connected with tcp/ip
  but with virtual port (Axon). VirtualRobot does not have its own senses or mucles. but it
  shares those of real MainRobot.
  Solution:
  - Robot knows its MainRobot role Robot. Mainrobot holder can be real MainRobot or Virtual Robot
  - Robot knows its Memory, choose this. So memory should have its semaphores
  - Implement Memory class
  
 - There is no reason why we still have MainRobot-class. Next step would be to
   move all specoal MainRobot-functionality ti Robot class. We can detact if
   we are MainRobot if level is 1 and We don't have parents. Study if VirtualRobot
   is only Robot with level 1, even if it has parent. 
  
  It has its own memory for Sensations. This means that also MainRobot should have its own memory,
  so we can't use global Memory any more.
  
  With VirtualMainRobot and Real MainRobot we can implement also robots in love, communicating with each other
  but relationship is not so real than real MainRobot + reamote MainRobot with independent senses and muscles.    


Todo
----

Solved OK
Maybe MainRobot stops, because someone deletes a Sensation that put om Axon,
but is not processed yet. We must find a way how to reserve and free Sensations.
Solution.
- Sensation,create sets owneship od Created Sensation to the caller Robot.
- Axon.put cleares Sensation of ther caller Robot ans set it to the Robot that will get that Sensation
- Robot.run.Axon.get gets Sensation, but ownership al already set to this Robot.
- At the and of run loop or Robot.process we release this Robots owenweship.
- Communication is special case, because it gets Sensation fron the Memory also.
  These sensation are processed by oernership same way as Axon-Process -loop.
-- reserve/release -> attach/detach() and it will be expanded to Robot.create,
   Axon, Robot.process.


Some times Robot just stops without any hint why. top shows 100% processor usage, but
nothing happens. Improve logging so, that each Robot shows last process() and sense()
call. Maybe we should also lof Axon traffic.
Last log
RaspberryPiCamera:2:Normal: isChangedImage final change 1289458 change > self.CHANGE_RANGE True
RaspberryPiCamera:2:Normal: sense self.getParent().getAxon().put(sensation) stream 190741
Create new sensation by pure parameters

if stopped from Visual
Meaning that MainRobot does not read its Axon any more

Meaning that we should put more logging in MainRobot
Last MainRobot log
Wall-E:1:Normal: Local robot Visual has capability for this, robot.getAxon().put(sensation)

Semephore problem, when deleting sensation from cache?
Yes not no.
All sensatyion cache operations should be protectted by semaphore. Now we protected only deletion from cahe,
but olther threads that read, will fail. In principle we should allow many readers, but only one writer thread.


Robot.id should be static, not dynamic. Make a method for that.

Implement memory handling with this
>>> import psutil
>>> mem = psutil.virtual_memory()
>>> mem
svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712, active=4748992512, inactive=2758115328, buffers=790724608, cached=3500347392, shared=787554304, slab=199348224)
>>>
>>> THRESHOLD = 100 * 1024 * 1024  # 100MB
>>> if mem.available <= THRESHOLD:
...     print("warning")
...
>>>

Add id for Robot.
Sensation would hold that id, when Robot Creates a new Sensation.
After that, Presence of Item.name can be handled by every Robot and MainRobot can handle
main presence. That way Item.name can be present in one Robot and if that Rpbot in under
MainRobot, Item.name is prent under that MainRobot etc. So if MainRobot in in network,
Sensation.Robot.id is changed to Sensation.MainRobot.id, when ttncferred by TCP SocketClient.
That way we can bind presence to a place, site, without knowing site location. Site is MainRobot.id.
- started

Robot is Thread. can we use thread id? There is not such.

Arrange Sensation Cache by Item.names also. Only one Item will be allowed per name.
But could this be sensible. At one moment we could have many sensations of one Item.name.
Would Items{} by Item.name be useful?

Communicate with Images
- We human say ans hear voices said
- Robot can also show exact image as it has seen it
- With Item.name we see/head what Rbot commucates ans givwe feedback +/-
-- We could also momment with voices we say
--- We could also comment with images, but this need that we have some libraty/Gallery etc. from where we give sensation to the Robot

Robot Item.name: people
  Robot Image (seen->show)  + --------!
  Robot Voice (heard->said) (-)------ ! --!
People log to present <-              !   !
  People like his/her picture --- ----!   !   
  People say Hello                        !
  - microphne button                      !
  People dislike a Voice he heard         !
     associated toa item.name -------------  
- Android-version

- Study Presence by Location -consept
  Maybe we should implement also some location group where this kind action is valid.

- Add ReactionToSensation-attribute to Sensation so if we we have many Robots in the network, then first can one reacts
  in Communication and also in TensorflowClassification to create new Sensations based on source Sensation(s). Other Robots
  must not react when or to some actions to like play a sound if some other Robot has done it already.

- (Main)Robot should be a Sensation or should include sensation(s) that have identity information
  and associations to them , meaning that Robot can feel of things (Sensation)
- (Main)Robot should talk as it's kind. Try that wall-E -kind Robot speeks for
  instance 2/3 rate ( meaning, that we use produce 2/3 sample amount of 1
  
- Robot should deliver sensations only to subRobots running
-- low priority, because problen comes, if some subrobot die and they don't if no developing problems.

Corrected
---------

OK It seems that Robot tcp routing is hard to test
OK -- For reason or another it seems that in same code is not possible test local and tcp Robot sensations routing.
   This should be possible and we have error in test code in SetUp and tearDown routiness.
   Had to set location back after test


OK Seems Communication may se set into infinite loop
it gets
[localhost]
who = Communication
sense_working_item = True
sense_sensory_voice = True
kind = Wall-E
robotid = 1593901694.7025573
location = Ubuntu

and produces
LongTerm.Item
LongTerm.Voice, there error found OK

OK When other host stops to answer, we will get huge number
father:1:Normal:Ubuntu: Sensations cache Not Forgottable robot SocketClient: ('192.168.117.247', 2000) Sensation Thu Jun  4 10:29:44 2020:1591227466.5333443:1591255771.647056:Working:Sense:Voice:::Neutral:Normal
Meaning that we can't forget these.


OK Introduce Robot with random own Voice, not first one as now.
Try same with Identification

OK Implement flat Memory, no MemoryType divided any more

OK Correct Communication configuration and accepting
OK, Wotking is correct
Make presentation changes as Sensory sensations, not Working,
because they are Sensory level.
-- OOps this was not good idea, because it leads to infinite loop in
   TensorflowClassification
Try Working

OK Configuration for Communication if Voice: Sensory, Out, Item: Working, Out 
Study Communication SensationType: should be intrested about speken Voice: Sensory, as well as seen Item; sensory?,
but not else? because Working is for processing and LongTerm is for remebering?


OK Don't associate Sensory.Feeling in Memory, but forget when done
- getMemorability for feeling is 0.0, so it will be deleted soon
- Memory does not associate Feelings

OK Simplyfy Communication much.
- When we don't get responce, we know present Item.named and original Voice that was chosen
  to be played. Feel negative with Iten.name and that original Voice

  And opposite way if we get an response,

OK Implement Sensationype.Feeling positive, negative

OK When we transfer Sensation with TCP as bytes, do we restore also id exactly same
in receiver that it was in sender?

OK Add Location to Configuration, Robot and Sensation.
OK all for this
When we use Capabilities, we should or also use Locations, but only by Direction.In Location, speak out, etc.,
but that we can't do, because capabilituis can't be separated by foreign Robots,
so make just or for Locations and test how it works. This  orred capabilities should be
sent to other part of tcp-connection.

Robots wll be organized by Location. One Location will include Robot group,
that is reasonable "human being", senses to create4 Senstion and muscles like
speak and show, move etc. and Memory. This group will also include Communication,
So this is Location based Awareness.
- When we hear, we hear in one Location,
- When we speak as respense, we speak to that Location we heard.
- Communication can give responce also based on other Locations heard Voices,
  so Memory is not Location based, but Sensation includes Location(s).
- Location(s) based Robots and Sesatioions are now
-- Microphone, Voice
-- Speaker, Voice
-- Camera, Image
-- Projector, Image
-- Communication, Voice, Image
-- TensorflowClassification, Item, presence
- They should all be owned by one or many  'MainRobot(s)', in one
or many computers. those 'MainRobots' also that have also Memory, so
Memory is somehow distributes and nor t only inside one 'MainRobot'.
So it seems that play with new parameter 'Location'. Seems that easiest way
is set up one Communication for one Robot group Location.
We coulkd also allow many Robot groups in one Location like Wall-E and Eva
sharing same senses and muscles. Implementation could use VirtualRobot or the normal
'MainRobots' in different computers. Start to build configuration where Ubuntu is 'Wall-E'
and raspberry is Eva' nas senses come from Raspberry. There can be two speakers,
one per computer fot debug reasons.
-- Seems that all SensationTypes should have this Locations property, not only
   Voice and Image, because it's hard to know what are common and what are Location
   based Sensations, so make all to be configurable in Robot.cfg
-- Presence should also be Location based
-- Routing: should we have a route max. Otherwise there will come too much trafffic in the network.

TODO Now
1) set locations from input Robots
2) in Communication Robot we should process all,
   but voices are set to locations fron inout
3) MainRobot level sends respond voices to original voice source.
4) Expand communication to vommunicate also with images, not waiting respomces yet.

Capabilities should have one level more, Location.
- Config and Capabilities have now locations, but no levels.
  Capabilities are valid in these locations, but not elsewhere
 


OK Should Identity send voices and images as 'Global'  and all robots route them
so all can hear them?
-- Yes, this is done to sel Location empty

OK Maybe Microphone captures voice at  time, when should not.
-- corrected creating and routing wrong sensation
OK Don't change Voice more than once. But this should be true, voice is changed in Playback.
-- But what is this? Voices should not be played or changes elsewhere than in Playback, so this shouls not be happened.
-- Original problem was with Playback ans Microphone, that allows Microphone record, when playback was on. Error was routing wrong Sensation to Microphone,
   as explained above.


OK Assign to most memorable assign, now we ignore assignation if max of assignations is reached.
Accept new assignation, but forget less memorable one.
- just removed limitation


OK model coco v1 can get (got once)

2020-02-28 18:59:09.755980: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:573] memory_optimizer failed: Deadline exceeded: memory_optimizer exceeded deadline., time = 563687.375ms.
tensorflow/core/grappler/optimizers/meta_optimizer.cc:573]

and TensorflowClassification will stop
-- not got any more, no critical

OK OOps
Sensations cache Not Forgottable robot SocketClient: ('192.168.117.247', 2000) number 1394
Not problem any more


Configuration info
------------------

With raspberry
- tensorflow Version: 1.14.0



  


What is done
------------

- Architecture, that is
-- devided into Robots that are implemented by Bobots, meaning
--- inheritance
---- Robot can be anywhere in one running instance
--- multihosted, Robots can be anywhere in the world, in any host
-- identity
--- Robot knows some details of itself
-- Virtual instances
--- this is for testing purposes
---- We can create sensations to our Real instance and study how it reacts
     meaning that we can finally start to implement thast our Robots gets Sensations
     that nake to fall in love, our final goal, make a Robot that can feel.
     But before that we have some to do.
-- Static LongTerm Memory
---- All whar Robot knows about this worlds in imlemented as Senmation class
    instances which are connected with each other. Anything other way is used.
    This momocs human memory. 
--- When a rub stoppes, we can save satus of memory and kload it back, when
    new run starts 
-- Item
--- This is a Sensation connected to other Sensation, just a name of a thing
    and a  score for this
-- Tensorflow classification to analyse Images Robot gets. This produces
   Items connected to subImages, cropped from camera Images is Classification
   results. This proces gives to Robot capability so see thing and give a name
   to the seen things and capability to connect other Sensations like voices
   to this Item. This mimics our brain main functionality: we are good at 
   processing visual things and that is most that our brains do. That will be
   main thing Robot will do, just process Images and think what it has seen.
   
- Make clearer git-directory order
-- 1) Wall-E
   Project-name
--- 2)Robot
    - Robot platform
---- 3) linux
     - linux specific files like starting service   
---- 3) python3
----- 4) Robot
      - python3 Robot module implementation
----- 4) package
      - package making scripts
      - package     
---- 3) android
     - android implementation
     

   
What is not done and needs implementation
-----------------------------------------

Robot.process
- divide into two phasess
-- processSensation(sensation)
--- if there is something in Axon, process it
    AlsaPlayback is ecaple of implementation and
    also LoggerRobot explayned below
-- produceSensation()
--- when Sensation are processed, we can sense: as a sense produce a sensation
    default implementation for this is empty, but AlsaMicrophone is an example of implementation,
    as well raspberryPiCamera
- Sensation
-- Sourcerobot

- LoggerRobot
-- interrested of everything
-- Logges current sensations by order
-- whows images seen by order, yhese are diffent we show
-- echoes voices heard, these are different than voices spoken

- LoadSpeakerRobot == AlsaAudioPlayBack, for communicating by Voices with Items
-- TODO howto implement Robot which code is same than other,
   but work and /etc directory is different?
--- We can implement links, same way than linux does
--- or just inherit a robot by import,
    this gives us possibility to separater technology (AlseSpearker) and role (mouth, logger)
- ViewerRobot, for communication by imeges with Items
- We can name of main class of a seen thing. Names are like 'person',
  'dining table', 'bicycle', 'tv' etc., but we cant yet identity things inside
  main class like 'John' is a 'person'. So Robot don't know id it sees 'John' or
  'Joan', it sees just 'person'.
-- implementing this needs
--- live Tesorflow model. We use now foren one. Learning means that we should
    find out characteristic things between 'John' and 'Joan'. These can be voice
    or some statistical thing in images like colors.
- Live Tensorflow model for voices. Image gives a name of seen thing. We connect
  name to voice heard same time thing has been seen. Most potential way to
  implement this kind Ternsorflow learning model for voices is use converting
  scalar voice information as one channel (blachwhite) image information as
  describet in Tensorflow voice analysing totorial pages.
- Expectations.
-- We can study statistically what is happening in hours of day,
  days in week, etc. and make Robot to wait that same kind thins happen and if
  Robot finds out that thing are going on in normal way, make Robot happy. We
  are implementing Feelings. We are near out goal are'nt we?
-- When something happens like we hear a voice identified to Sensorflow to
   a class name, meaning on Item with this name, connected to other kind Sensation
   like Image, we can implement expectation: if something happens, then it
   is expected to happen also something else. Here we can make our Robot happy
   if Robot is right and it has leaned something of thos world. We are even closer
   to our goal, are'nt we?
   
Dependencies
------------  

dependencies: for pillow (pip3 install pillow)
sudo apt-get install libjpeg-dev -y
sudo apt-get install zlib1g-dev -y
sudo apt-get install libfreetype6-dev -y
sudo apt-get install liblcms1-dev -y
sudo apt-get install libopenjp2-7 -y
sudo apt-get install libtiff5 -y

sudo pip3 install pillow


# senses
sudo pip3 install pyalsaaudio
# needs to set AlsaAudioMicrophonw/etc/Robot.cfg
## default:CARD=Device

sudo pip3 install picamera
Camra must be enable from raspi-config

numpy needs
sudo apt-get install python-dev libatlas-base-dev

sudo pip3 install numpy

MainRobot needs
sudo pip3 install python-daemon
sudo pip3 install lockfile
sudo pip3 install psutil

Visual needs
sudo pip3 install wxpython
raspberry pi
- https://wiki.wxpython.org/BuildWxPythonOnRaspberryPi
sudo apt-get install dpkg-dev build-essential libjpeg-dev libtiff-dev libsdl1.2-dev libgstreamer-plugins-base0.10-dev libnotify-dev freeglut3 freeglut3-dev libwebkitgtk-dev libghc-gtk3-dev libwxgtk3.0-gtk3-dev
wget https://files.pythonhosted.org/packages/b9/8b/31267dd6d026a082faed35ec8d97522c0236f2e083bf15aff64d982215e1/wxPython-4.0.7.post2.tar.gz
tar xf wxPython-4.0.7.post2.tar.gz
cd wxPython-4.0.7.post2
sudo pip3 install -r requirements.txt
--- parsberry pi 3b with 1 MB ram could compile if 2 threads
python3 build.py build bdist_wheel --jobs=2
sudo pip3 install dist/wxPython-4.0.7.post2-cp37-cp37m-linux_armv7l.whl

(pip3 install pyyaml # Required to save tensorflow models in YAML format
 not yet implemented ready)

raspberry pi 3b+ Buster Lite
Needs pip3
sudo apt install python3-pip

Tensorflow with Lite, officional package is broken for Lite
python 3.5
https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp35-none-linux_armv7l.whl

python 3.7 (buster)
sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl
gives tensorflow==2.1, which is cuttent when this was written 
(if not work, old instruction
(https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp37-none-linux_armv7l.whl
tensorflow==1.8

Ubuntu 18.4
tensorflow==1.5? -> Last version to work with virtual machine
Ubuntu 14.04
sudo -H pip3 install --upgrade tensorflow --ignore-installed six
-- will get tensorflow 1.13
sudo -H pip3 install --upgrade tensorflow
-- will get tensorflow 1.14
requires
google.protobuf
matplotlib==2.1.0
requires
- sudo apt-get install libfreetype6-dev

OK also you need models from data, application should download these, but the is an error. TODO

Hardware
--------

Study these devices
- Sipeed MAix
-- Support MicroPython on M1
-- out of stock
- HUSKYLENS
-- kickstart,
-- emailed for supoort how to use and for free sample
- Google AI Camera set
- pixy2
-- color signature
-- no tensorflow
- Ai-Thinker ESP32-CAM
-- low
-- face recognition and detection

Develop
------
Done
----

OK Sensations cache Not Forgottable robot Wall-E number 797
Seems that we can't forget now!

OK Finalize conditional associations with proper variables


OK Not needed any more
  starting MainRobot with Tensorflow by command line needs now
  PYTHONPATH=/home/reijo/models/research python3 MainRobot.py --start
  and you should copy git/Tensorflow/models/research to that place

OK Seems that capability sharing is not working: Wally does not know that PV can handle Voice In (voices to be spoken)
   Seems that capabilities are transferred OK

OK  We have setting and support for 1-channel microprone now
AlsaAudioMicrophone produces mono, 1-channel file with raspberry USB-microphone
even if setting are 2-channels. Still output is expected to have 2-channels.

OK  Check AlsaAudioMicrophonePlayback logic.
Should read microphone all the time, so we get right average voice level.
Should not be interesting if items are present.
If voice found, then should read as long as voice is on, so we get nice voice.

OK Found fast enoung LITE model and a way to run it for raspberry.
OK TODO check logic and test

- DONE permanent=False -attribute to sensations. This is not set as bytes, but is local property
OK Add not-saveable attribute isForgettable (int)/ reserve(),release() inUse(int) to Senation,
so these Sensation will be reachable as long as Robot.process will process them.
 
OK Start to use LongTerm-memory, when we get an response, so we remember things that should be rembembered.
In Memory-version is ready, but we should study also datavase version.

OK  File "/home/reijo/Wall-E/Robot/Sensation.py", line 1506, in removeAssociation
    del sensation.associations[i]
OK IndexError: list assignment index out of range
 
OK AlsaAudioPlayback: Normalize voices to play
OK Voice Kind cannot be local only, because Communinication and AlsaAudioBroadcast can be in different servers

OK Study memory usage
- import resource
- resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
Deprfecated Change Sensation.create so, that it returns an old Sensation if requested data is close enough.
            This would help also to memory problems, even id we still must implemnt some memory  management.
            Maybe not good idea. Or we should change old  Sensation with new one. But this is not simple to implement.

OK In raspberry application easily runs out of menory
OK -- should implement some method to remove more efficiently not so important sensations

OK def tracePresents(self, sensation):
- to Robot.method
- used in MainRobot
- OK We know present item-names now 

OK Robot.process to support asscociation
-- with that we can ass Feeling to processing (for instance speeking out with a feeling)
-- there is no implementation for this

OK Association to support presence

OK Develop VirtualRobot
- acts as Item.name
- produces Images as Camera does
- Produces Voices as AlsaMicroprone does
- Listens if Robot speaks and answers
- checks if is is marked to be present or not

OK Finalize presense
- Presence works in Tensorflow and AlsaMictophone, associating voices and Images to Item.name present

OK Comminication need to be implented also with supporting presense
- do this test drive
-- support testTensorlow. so real Image and Item sensations are created
-- Voices should be added manually, because AlsaAudioMicrophone can't be test.driven yelt, we can't emulate real devices

OK SocketServer keeps running on error
   SocketServer:('192.168.117.7', 2000):3:Interrupted: run: self.sock.recv(sensation_length) Interrupted error ('192.168.117.7', 2000) [Errno 9] Bad file descriptor


OK Make helper scrits to start and stop MainRobot and write to /tmp/files
   We have servoce-scrip now

OK SocketClient/SocketServer
  Check that it tells only pure Capabilities, not Capabilities from remote Robots
  Other way we get too much traffic. Architecture will be  1:n socket-view
   
OK Tensorflow
  Implement presense so that it report only presense changes
  Timestamp should be same than the original Image timestamp
  
OK Assosiation
  Keep status of present Item.names and assosiate only to present ones, not
  all the stuff.
  Time-based logic is not needed any more
  
OK Sensation
  Report hosts from sensatios is driven
  
 
OK AlsaAudiMicrophonePlayback does not stop. Add stop-handling


OK Implement manualThreading
- very light step that can be run, just sense() to queue or precess() from queue
  -- study how to do this
  -- priority of Robots
  -

OK Created 3 SocketServer and 3 SockertClient threads/instances. 1 +1 is enough.

