Critical
--------

With raspberry
- tensorflow Version: 1.14.0

- model coco v1 can get (got once)

2020-02-28 18:59:09.755980: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:573] memory_optimizer failed: Deadline exceeded: memory_optimizer exceeded deadline., time = 563687.375ms.
tensorflow/core/grappler/optimizers/meta_optimizer.cc:573]

and TensorflowClassification will stop



Develop
-------

Todo
----

Oops, Connection was forgotten to move!

Maybe MainRobot stops, because someone deletes a Sensation that put om Axon,
but is not processed yet. We must find a way how to reserve and free Sensations.
Solution.
- Sensation,create sets sonweship od Created Sensation to the caller Robot.
- Axon.put cleares Sensation of ther caller Robot ans set it to the Robot that will get that Sensation
- Robot.run.Axon.get gets Sensation, but ownership al already set to this Robot.
- At the and of run loop or Robot.process we release this Robots owenweship.
- Communication is special case, because it gets Sensation fron the Memory also.
  These sensation are processed by oernership same way as Axon-Process -loop.
-- reserve/release -> attach/detach() and it will be expanded to Robot.create,
   Axon, Robot.process.

Add Robot roles
- Remote -> RemoteSubRobot is part of remote MainRobot, meaning that it has only senses/mucle Robot
  that report to tja main Robot or get Sensation to mucles from remote Mainrobot
  even though this Sites MainRobot has MainRobot instance, that instance get its identity from the remote MainRobot
  but don't read any identity data from its directories.
- RemoteMainRobot This mainRobot think that other remore is independent MainRobot.
  Remote MainRobot has ts own identity, so it speaks as its own and shows images as its own.
  Two MainRobots cam coversate each with other and even be in love with each other.
  So we can make Wall-E and Eve-robots to be in love with each other.
- VirtualMainRobot is just like RemoteMainRobot but it is not connected with tcp/ip
  but with virtual port (Axon). VirtualRobot does not have its own senses or mucles. but it
  shares those of real MainRobot. Its has its own memory for Sensations.
  With VirtualMainRobot and Real MainRobot we can impllement also robots in love, communicating with each other
  but relationship is not so real than real MainRobot + reamote MainRobot with independent senses and muscles.    


Some times Robot just stops without any hint why. top shows 100% processor usage, but
nothing happens. Improve logging so, that each Robot shows last process() and sense()
call. Maybe we should also lof Axon traffic.
Last log
RaspberryPiCamera:2:Normal: isChangedImage final change 1289458 change > self.CHANGE_RANGE True
RaspberryPiCamera:2:Normal: sense self.getParent().getAxon().put(sensation) stream 190741
Create new sensation by pure parameters

if stopped from Visual
Meaning that MainRobot does not read its Axon any more

Meaning that we should put more logging in MainRobot
Last MainRobot log
Wall-E:1:Normal: Local robot Visual has capability for this, robot.getAxon().put(sensation)

Semephore problem, when deleting sensation from cache?
Yes not no.
All sensatyion cache operations should be protectted by semaphore. Now we protected only deletion from cahe,
but olther threads that read, will fail. In principle we should allow many readers, but only one writer thread.


Robot.id should be static, not dynamic. Make a method for that.

Implement memory handling with this
>>> import psutil
>>> mem = psutil.virtual_memory()
>>> mem
svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712, active=4748992512, inactive=2758115328, buffers=790724608, cached=3500347392, shared=787554304, slab=199348224)
>>>
>>> THRESHOLD = 100 * 1024 * 1024  # 100MB
>>> if mem.available <= THRESHOLD:
...     print("warning")
...
>>>

Add id for Robot.
Sensation would hold that id, when Robot Creates a new Sensation.
After that, Presence of Item.name can be handled by every Robot and MainRobot can handle
main presence. That way Item.name can be present in one Robot and if that Rpbot in under
MainRobot, Item.name is prent under that MainRobot etc. So if MainRobot in in network,
Sensation.Robot.id is changed to Sensation.MainRobot.id, when ttncferred by TCP SocketClient.
That way we can bind presence to a place, site, without knowing site location. Site is MainRobot.id.
- started

Robot is Thread. can we use thread id? There is not such.

Arrange Sensation Cache by Item.names also. Only one Item will be allowed per name.
But could this be sensible. At one moment we could have many sensations of one Item.name.
Would Items{} by Item.name be useful?

Communicate with Images
- We human say ans hear voices said
- Robot can also show exact image as it has seen it
- With Item.name we see/head what Rbot commucates ans givwe feedback +/-
-- We could also momment with voices we say
--- We could also comment with images, but this need that we have some libraty/Gallery etc. from where we give sensation to the Robot

Robot Item.name: people
  Robot Image (seen->show)  + --------!
  Robot Voice (heard->said) (-)------ ! --!
People log to present <-              !   !
  People like his/her picture --- ----!   !   
  People say Hello                        !
  - microphne button                      !
  People dislike a Voice he heard         !
     associated toa item.name -------------  
- Android-version

- Study Presence by Location -consept
  Maybe we should implement also some location group where this kind action is valid.

- Add ReactionToSensation-attribute to Sensation so if we we have many Robots in the network, then first can one reacts
  in Communication and also in TensorflowClassification to create new Sensations based on source Sensation(s). Other Robots
  must not react when or to some actions to like play a sound if some other Robot has done it already.

- (Main)Robot should be a Sensation or should include sensation(s) that have identity information
  and associations to them , meaning that Robot can feel of things (Sensation)
- (Main)Robot should talk as it's kind. Try that wall-E -kind Robot speeks for
  instance 2/3 rate ( meaning, that we use produce 2/3 sample amount of 1
  
- Robot should deliver sensations only to subRobots running
-- low priority, because problen comes, if some subrobot die and they don't if no developing problems.  
  


What is done
------------

- Architecture, that is
-- devided into Robots that are implemented by Bobots, meaning
--- inheritance
---- Robot can be anywhere in one running instance
--- multihosted, Robots can be anywhere in the world, in any host
-- identity
--- Robot knows some details of itself
-- Virtual instances
--- this is for testing purposes
---- We can create sensations to our Real instance and study how it reacts
     meaning that we can finally start to implement thast our Robots gets Sensations
     that nake to fall in love, our final goal, make a Robot that can feel.
     But before that we have some to do.
-- Static LongTerm Memory
---- All whar Robot knows about this worlds in imlemented as Senmation class
    instances which are connected with each other. Anything other way is used.
    This momocs human memory. 
--- When a rub stoppes, we can save satus of memory and kload it back, when
    new run starts 
-- Item
--- This is a Sensation connected to other Sensation, just a name of a thing
    and a  score for this
-- Tensorflow classification to analyse Images Robot gets. This produces
   Items connected to subImages, cropped from camera Images is Classification
   results. This proces gives to Robot capability so see thing and give a name
   to the seen things and capability to connect other Sensations like voices
   to this Item. This mimics our brain main functionality: we are good at 
   processing visual things and that is most that our brains do. That will be
   main thing Robot will do, just process Images and think what it has seen.
   
- Make clearer git-directory order
-- 1) Wall-E
   Project-name
--- 2)Robot
    - Robot platform
---- 3) linux
     - linux specific files like starting service   
---- 3) python3
----- 4) Robot
      - python3 Robot module implementation
----- 4) package
      - package making scripts
      - package     
---- 3) android
     - android implementation
     

   
What is not done and needs implementation
-----------------------------------------

Robot.process
- divide into two phasess
-- processSensation(sensation)
--- if there is something in Axon, process it
    AlsaPlayback is ecaple of implementation and
    also LoggerRobot explayned below
-- produceSensation()
--- when Sensation are processed, we can sense: as a sense produce a sensation
    default implementation for this is empty, but AlsaMicrophone is an example of implementation,
    as well raspberryPiCamera
- Sensation
-- Sourcerobot

- LoggerRobot
-- interrested of everything
-- Logges current sensations by order
-- whows images seen by order, yhese are diffent we show
-- echoes voices heard, these are different than voices spoken

- LoadSpeakerRobot == AlsaAudioPlayBack, for communicating by Voices with Items
-- TODO howto implement Robot which code is same than other,
   but work and /etc directory is different?
--- We can implement links, same way than linux does
--- or just inherit a robot by import,
    this gives us possibility to separater technology (AlseSpearker) and role (mouth, logger)
- ViewerRobot, for communication by imeges with Items
- We can name of main class of a seen thing. Names are like 'person',
  'dining table', 'bicycle', 'tv' etc., but we cant yet identity things inside
  main class like 'John' is a 'person'. So Robot don't know id it sees 'John' or
  'Joan', it sees just 'person'.
-- implementing this needs
--- live Tesorflow model. We use now foren one. Learning means that we should
    find out characteristic things between 'John' and 'Joan'. These can be voice
    or some statistical thing in images like colors.
- Live Tensorflow model for voices. Image gives a name of seen thing. We connect
  name to voice heard same time thing has been seen. Most potential way to
  implement this kind Ternsorflow learning model for voices is use converting
  scalar voice information as one channel (blachwhite) image information as
  describet in Tensorflow voice analysing totorial pages.
- Expectations.
-- We can study statistically what is happening in hours of day,
  days in week, etc. and make Robot to wait that same kind thins happen and if
  Robot finds out that thing are going on in normal way, make Robot happy. We
  are implementing Feelings. We are near out goal are'nt we?
-- When something happens like we hear a voice identified to Sensorflow to
   a class name, meaning on Item with this name, connected to other kind Sensation
   like Image, we can implement expectation: if something happens, then it
   is expected to happen also something else. Here we can make our Robot happy
   if Robot is right and it has leaned something of thos world. We are even closer
   to our goal, are'nt we?
   
Dependencies
------------  

dependencies: for pillow (pip3 install pillow)
sudo apt-get install libjpeg-dev -y
sudo apt-get install zlib1g-dev -y
sudo apt-get install libfreetype6-dev -y
sudo apt-get install liblcms1-dev -y
sudo apt-get install libopenjp2-7 -y
sudo apt-get install libtiff5 -y

sudo pip3 install pillow


# senses
sudo pip3 install pyalsaaudio
# needs to set AlsaAudioMicrophonw/etc/Robot.cfg
## default:CARD=Device

sudo pip3 install picamera
Camra must be enable from raspi-config

numpy needs
sudo apt-get install python-dev libatlas-base-dev

sudo pip3 install numpy

MainRobot needs
sudo pip3 install python-daemon
sudo pip3 install lockfile
sudo pip3 install psutil

Visual needs
sudo pip3 install wxpython
raspberry pi
- https://wiki.wxpython.org/BuildWxPythonOnRaspberryPi
sudo apt-get install dpkg-dev build-essential libjpeg-dev libtiff-dev libsdl1.2-dev libgstreamer-plugins-base0.10-dev libnotify-dev freeglut3 freeglut3-dev libwebkitgtk-dev libghc-gtk3-dev libwxgtk3.0-gtk3-dev
wget https://files.pythonhosted.org/packages/b9/8b/31267dd6d026a082faed35ec8d97522c0236f2e083bf15aff64d982215e1/wxPython-4.0.7.post2.tar.gz
tar xf wxPython-4.0.7.post2.tar.gz
cd wxPython-4.0.7.post2
sudo pip3 install -r requirements.txt
--- parsberry pi 3b with 1 MB ram could compile if 2 threads
python3 build.py build bdist_wheel --jobs=2
sudo pip3 install dist/wxPython-4.0.7.post2-cp37-cp37m-linux_armv7l.whl

(pip3 install pyyaml # Required to save tensorflow models in YAML format
 not yet implemented ready)

raspberry pi 3b+ Buster Lite
Needs pip3
sudo apt install python3-pip

Tensorflow with Lite, officional package is broken for Lite
python 3.5
https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp35-none-linux_armv7l.whl

python 3.7 (buster)
sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl
gives tensorflow==2.1, which is cuttent when this was written 
(if not work, old instruction
(https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp37-none-linux_armv7l.whl
tensorflow==1.8

Ubuntu 18.4
tensorflow==1.5? -> Last version to work with virtual machine
Ubuntu 14.04
sudo -H pip3 install --upgrade tensorflow --ignore-installed six
-- will get tensorflow 1.13
sudo -H pip3 install --upgrade tensorflow
-- will get tensorflow 1.14
requires
google.protobuf
matplotlib==2.1.0
requires
- sudo apt-get install libfreetype6-dev

OK also you need models from data, application should download these, but the is an error. TODO

Hardware
--------

Study these devices
- Sipeed MAix
-- Support MicroPython on M1
-- out of stock
- HUSKYLENS
-- kickstart,
-- emailed for supoort how to use and for free sample
- Google AI Camera set
- pixy2
-- color signature
-- no tensorflow
- Ai-Thinker ESP32-CAM
-- low
-- face recognition and detection

Develop
------
Done
----

OK Not needed any more
  starting MainRobot with Tensorflow by command line needs now
  PYTHONPATH=/home/reijo/models/research python3 MainRobot.py --start
  and you should copy git/Tensorflow/models/research to that place

OK Seems that capability sharing is not working: Wally does not know that PV can handle Voice In (voices to be spoken)
   Seems that capabilities are transferred OK

OK  We have setting and support for 1-channel microprone now
AlsaAudioMicrophone produces mono, 1-channel file with raspberry USB-microphone
even if setting are 2-channels. Still output is expected to have 2-channels.

OK  Check AlsaAudioMicrophonePlayback logic.
Should read microphone all the time, so we get right average voice level.
Should not be interesting if items are present.
If voice found, then should read as long as voice is on, so we get nice voice.

OK Found fast enoung LITE model and a way to run it for raspberry.
OK TODO check logic and test

- DONE permanent=False -attribute to sensations. This is not set as bytes, but is local property
OK Add not-saveable attribute isForgettable (int)/ reserve(),release() inUse(int) to Senation,
so these Sensation will be reachable as long as Robot.process will process them.
 
OK Start to use LongTerm-memory, when we get an response, so we remember things that should be rembembered.
In Memory-version is ready, but we should study also datavase version.

OK  File "/home/reijo/Wall-E/Robot/Sensation.py", line 1506, in removeAssociation
    del sensation.associations[i]
OK IndexError: list assignment index out of range
 
OK AlsaAudioPlayback: Normalize voices to play
OK Voice Kind cannot be local only, because Communinication and AlsaAudioBroadcast can be in different servers

OK Study memory usage
- import resource
- resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
Deprfecated Change Sensation.create so, that it returns an old Sensation if requested data is close enough.
            This would help also to memory problems, even id we still must implemnt some memory  management.
            Maybe not good idea. Or we should change old  Sensation with new one. But this is not simple to implement.

OK In raspberry application easily runs out of menory
OK -- should implement some method to remove more efficiently not so important sensations

OK def tracePresents(self, sensation):
- to Robot.method
- used in MainRobot
- OK We know present item-names now 

OK Robot.process to support asscociation
-- with that we can ass Feeling to processing (for instance speeking out with a feeling)
-- there is no implementation for this

OK Association to support presence

OK Develop VirtualRobot
- acts as Item.name
- produces Images as Camera does
- Produces Voices as AlsaMicroprone does
- Listens if Robot speaks and answers
- checks if is is marked to be present or not

OK Finalize presense
- Presence works in Tensorflow and AlsaMictophone, associating voices and Images to Item.name present

OK Comminication need to be implented also with supporting presense
- do this test drive
-- support testTensorlow. so real Image and Item sensations are created
-- Voices should be added manually, because AlsaAudioMicrophone can't be test.driven yelt, we can't emulate real devices

OK SocketServer keeps running on error
   SocketServer:('192.168.117.7', 2000):3:Interrupted: run: self.sock.recv(sensation_length) Interrupted error ('192.168.117.7', 2000) [Errno 9] Bad file descriptor


OK Make helper scrits to start and stop MainRobot and write to /tmp/files
   We have servoce-scrip now

OK SocketClient/SocketServer
  Check that it tells only pure Capabilities, not Capabilities from remote Robots
  Other way we get too much traffic. Architecture will be  1:n socket-view
   
OK Tensorflow
  Implement presense so that it report only presense changes
  Timestamp should be same than the original Image timestamp
  
OK Assosiation
  Keep status of present Item.names and assosiate only to present ones, not
  all the stuff.
  Time-based logic is not needed any more
  
OK Sensation
  Report hosts from sensatios is driven
  
 
OK AlsaAudiMicrophonePlayback does not stop. Add stop-handling


OK Implement manualThreading
- very light step that can be run, just sense() to queue or precess() from queue
  -- study how to do this
  -- priority of Robots
  -

OK Created 3 SocketServer and 3 SockertClient threads/instances. 1 +1 is enough.

