Critical
--------

OOps
Sensations cache Not Forgottable robot SocketClient: ('192.168.117.247', 2000) number 1394

With raspberry
- tensorflow Version: 1.14.0

- model coco v1 can get (got once)

2020-02-28 18:59:09.755980: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:573] memory_optimizer failed: Deadline exceeded: memory_optimizer exceeded deadline., time = 563687.375ms.
tensorflow/core/grappler/optimizers/meta_optimizer.cc:573]

and TensorflowClassification will stop



Develop
-------

Assign to most memorable assign, now we ignore assignation if max of assignations is reached.
Accept new assignation, but forget less memorable one.

Do we try to reconnect to hosts, if connection is lost.

Should Identity send voiceds and images as 'Global'  and all robots route them
so all can hear them?


OK When we transfer Sensation with TCP as bytes, do we restore also id exactly same
in receiver that it was in sender?

Add Location to Configuration, Robot and Sensation.
Robots wll be organized by Location. One Location will include Robot group,
that is reasonable "human being", senses to create4 Senstion and muscles like
speak and show, move etc. and Memory. This group will also include Communication,
So this is Location based Awareness.
- When we hear, we hear in one Location,
- When we speak as respense, we speak to that Location we heard.
- Communication can give responce also based on other Locations heard Voices,
  so Memory is not Location based, but Sensation includes Location(s).
- Location(s) based Robots and Sesatioions are now
-- Microphone, Voice
-- Speaker, Voice
-- Camera, Image
-- Projector, Image
-- Communication, Voice, Image
-- TensorflowClassification, Item, presence
- They should all ne owned by one or many  'MainRobot(s)', in one
or many computers. those 'MainRobots' also that have also Memory, so
Memory is somehow distributes and nor t only inside one 'MainRobot'.
So it seems that play with new parameter 'Location'. Seems that easiest way
is set up one Communication for one Robot group LOcation.
We coulkd also allow manu Rbo groups in one Location like Wall-E and Eva
sharing same senses and muscles. Implementation could use VirtualRobot or the normal
'MainRobots' in different computers. Start to build configuration where Ubuntu is 'Wall-E'
and raspberry is Eva' nas senses come from Raspberry. There can be two speakers,
one per computer fot debug reasons.
-- Seems that all SensationTypes should have this Locations property, not only
   Voice and Image, because it's hard to jnow whar are common and what are Location
   based Sensation, so make all to be configurable in Robot.cfg
-- Presence should also be Location based

TODO Now
1) set locations from iwput Robots
2) in Communication Robot we should process all,
   but voices are set to locations fron inout
3) MainRobot level sends respond voices to original voice source.
4) Expand communication to vommunicate also with images, not waiting respomces yet.

Capabilities should have one level more, Location.
- Config and Capabilities have now locations, but no levels.
  Capabilities are valid in these locations, but lot elsewhere
 
Next Step would be to implement Awareness-type Robot. This is "new MainRobot
with new name", but it will include missing Awareness-features, when
technical MainRobot-features are moved to Robot-common implementation.
Awareness includes:
- Memory
- Feeling: mood of this whore Robot
- Activitylevel
- Location

OK Simplyfy Communication much.
When we don't get responce, we know present Item.named and original Voice that was chosen
to be played.  played. Feel negative with Iten.name and that original Voice

And opposite way if we get an response,

Implement alsa Sensationype.Feeling positive, negative
Less Logging to Visual
Don't associate Feeling in Memory, but forget when done

OK Next step will be study and implement Association in Axon, Robot.process in memory level Robots, meaning MainRobot level. Association has score and feeling, so syudy how to implemt 'feel better,
or feel worse about fron subject to object sensation.
- score Sensation.Assosiation ->  Sentation.Item feature
OK But Needs cleanup
OK Cleanup
Still change this as Feeling Sensationtype to be clear. This is always Feeling beetween Sensations
Assosiation as SensationType
- Assisiations handling has beeb there, but no as SensationType.
- Actuaslly we need two parameters, thw Sensations to be associated,
  This type sensation must always be Sensory MemoryType

Delete not forgettable sensation before pickle, because we can't pickle them because they are associated with Robot. Anyway, thay are too old or under process, so they are obsolete.

Virtual Robots Visual does not start always.
X-client Probots Visual does not start very quicly always.

It is not good idea to change Sensation processing Robot. It should be what it is.


Identity is hard to stop. How to awake from sleep.

When communicating, return also bestImuonimpia, kun mieloestäni laitoin kuviot ihan oikein, mutta ingrespä oli toista mieltä.age for Item.

Maybe Identiyy should be a Ropot thats runs once and then stops.
That way we could stop Main Robot
or maybe we could brutally kill Identity-process, that is sleeping by thread methods.

Show present items in Visual
identity sensations as Sensory MemoryType
tellIdentity for Robot functionality and VirtualRobot is only Role, not class
(tell)Identity as SubRobot, so it can wait without doing harm to Visual,
when main thread does not sleep?

Robot should have selfSensation of type item ans name Robots name.
With that we can describe Robot, what it has been seen and heard (out)
and what it has said and shown (what it looks like to others). A satrtsup it shares this
information to others.


Logical MainRobot should have only one Communication in its congiguration.
So when Logical MainRobot has many sites in its configuration its must not have many Communications
Virtual Robot should have one Communication, TensoflowClassification abs Visual and
it should borrow MicroPhone, Speaker, Cameresources from other. How this borrowing is done?
Same question is with MainRobot that does not have those resources of its own.
Now we have definition of MainRobot to down in one site, Maybe we should also define
configuration of borrowed up direction fron network or for virtual Robot direct in one site.


Forget Virtual Robot Sensations
Forget Robot Sensations
Valler Visual does noy stop

wx does not work well with another MainFrame with another Visual
[xcb] Unknown request in queue while dequeuing

(Robot.py:9815): Gdk-WARNING **: Robot.py: Fatal IO error 11 (Resource temporarily unavailable) on X server :0.

[xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
[xcb] Aborting, sorry about that.


Visial can be heavy. When stating many of them, delay and use Axon between Rpbot and wx-processes
to transfer Sensation and just awake wx with wx-metdod.

OK Wall-E:1:Starting: pickle.load(f) Exception Ran out of input

Associatian can be removed, because Memory has functionality as build in.

OK Implement Robot.isRunning also or use thread method. We nead that in many places.
Not needed in many places, only when we wait, that processes are realy stopped.


Fix SocketClient NotForgottables. It still can't forget in some situations
Move Robot-Association functionality tp Memory.
Memory is a place, that otganises this kind things.

OK Check Alsa-Robots.
Check if we echo Sensation to original source.

Organize Robots by MainRobot groups

Principles from lowe level Robot to higher ones

Each Sense-type Robot can belong to one or more Groups. It can be sensible, if
MainRobot and VirtualRobot share same Senses and Mucles. VirtualRobot does not have
it own lower level Seses or Muscles. This saring should be done by MainRobot
which is a brigde to Real Senses and Muscles of Virtual Robot, so VirtualRobot knows only its MainRobot
which has capabilities. For VirtualRobot this functiolality is already implemented.

MainRobot is ownwer of its Senses as Mucles as a Group. MainRobot can also be a member of group.
That way Group must have only one MainRobot, that has Memory, but all MainRobots group members
shjare this Memory. This way group has one shared Memory and MainRobot that has created that Memory
is a brighe to all subrobots.



Each Robot should belong to one exact Robot group, that has one MainRobot.
MainRobot must have a name, so groupsa name is MainRobots name.
Robot can be started in one site or other site. Other site has also MainRobot but it should
be handled as bridge/Nomrmal robot. Study if we can implement MainRobot as Robot,
so we could implement all this by one Robot class. Earlier problem was TVPServer, SocketServer,
Socketclient implementation. If that can't be done easily, rename MainRobot as BridgeRobot
and still start Robot from Robot-class, not from MainRobot class. AnyWay, that way Robot gets its role from
cconfifurayion ans Level. Level 1 Robots can/should have bridge(s).


Add Robot roles
- Remote -> RemoteSubRobot is part of remote MainRobot, meaning that it has only senses/mucle Robot
  that report to tja main Robot or get Sensation to mucles from remote Mainrobot
  even though this Sites MainRobot has MainRobot instance, that instance get its identity from the remote MainRobot
  but don't read any identity data from its directories.
- RemoteMainRobot This mainRobot think that other remore is independent MainRobot.
  Remote MainRobot has ts own identity, so it speaks as its own and shows images as its own.
  Two MainRobots cam coversate each with other and even be in love with each other.
  So we can make Wall-E and Eve-robots to be in love with each other.

- VirtualMainRobot is just like RemoteMainRobot but it is not connected with tcp/ip
  but with virtual port (Axon). VirtualRobot does not have its own senses or mucles. but it
  shares those of real MainRobot.
  Solution:
  - Robot knows its MainRobot role Robot. Mainrobot holder can be real MainRobot or Virtual Robot
  - Robot knows its Memory, choose this. So memory should have its semaphores
  - Implement Memory class
  
 - There is no reason why we still have MainRobot-class. Next step would be to
   move all specoal MainRobot-functionality ti Robot class. We can detact if
   we are MainRobot if level is 1 and We don't have parents. Study if VirtualRobot
   is only Robot with level 1, even if it has parent. 
  
  It has its own memory for Sensations. This means that also MainRobot should have its own memory,
  so we can't use global Memory any more.
  
  With VirtualMainRobot and Real MainRobot we can implement also robots in love, communicating with each other
  but relationship is not so real than real MainRobot + reamote MainRobot with independent senses and muscles.    


Todo
----

Solved OK
Maybe MainRobot stops, because someone deletes a Sensation that put om Axon,
but is not processed yet. We must find a way how to reserve and free Sensations.
Solution.
- Sensation,create sets owneship od Created Sensation to the caller Robot.
- Axon.put cleares Sensation of ther caller Robot ans set it to the Robot that will get that Sensation
- Robot.run.Axon.get gets Sensation, but ownership al already set to this Robot.
- At the and of run loop or Robot.process we release this Robots owenweship.
- Communication is special case, because it gets Sensation fron the Memory also.
  These sensation are processed by oernership same way as Axon-Process -loop.
-- reserve/release -> attach/detach() and it will be expanded to Robot.create,
   Axon, Robot.process.


Some times Robot just stops without any hint why. top shows 100% processor usage, but
nothing happens. Improve logging so, that each Robot shows last process() and sense()
call. Maybe we should also lof Axon traffic.
Last log
RaspberryPiCamera:2:Normal: isChangedImage final change 1289458 change > self.CHANGE_RANGE True
RaspberryPiCamera:2:Normal: sense self.getParent().getAxon().put(sensation) stream 190741
Create new sensation by pure parameters

if stopped from Visual
Meaning that MainRobot does not read its Axon any more

Meaning that we should put more logging in MainRobot
Last MainRobot log
Wall-E:1:Normal: Local robot Visual has capability for this, robot.getAxon().put(sensation)

Semephore problem, when deleting sensation from cache?
Yes not no.
All sensatyion cache operations should be protectted by semaphore. Now we protected only deletion from cahe,
but olther threads that read, will fail. In principle we should allow many readers, but only one writer thread.


Robot.id should be static, not dynamic. Make a method for that.

Implement memory handling with this
>>> import psutil
>>> mem = psutil.virtual_memory()
>>> mem
svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712, active=4748992512, inactive=2758115328, buffers=790724608, cached=3500347392, shared=787554304, slab=199348224)
>>>
>>> THRESHOLD = 100 * 1024 * 1024  # 100MB
>>> if mem.available <= THRESHOLD:
...     print("warning")
...
>>>

Add id for Robot.
Sensation would hold that id, when Robot Creates a new Sensation.
After that, Presence of Item.name can be handled by every Robot and MainRobot can handle
main presence. That way Item.name can be present in one Robot and if that Rpbot in under
MainRobot, Item.name is prent under that MainRobot etc. So if MainRobot in in network,
Sensation.Robot.id is changed to Sensation.MainRobot.id, when ttncferred by TCP SocketClient.
That way we can bind presence to a place, site, without knowing site location. Site is MainRobot.id.
- started

Robot is Thread. can we use thread id? There is not such.

Arrange Sensation Cache by Item.names also. Only one Item will be allowed per name.
But could this be sensible. At one moment we could have many sensations of one Item.name.
Would Items{} by Item.name be useful?

Communicate with Images
- We human say ans hear voices said
- Robot can also show exact image as it has seen it
- With Item.name we see/head what Rbot commucates ans givwe feedback +/-
-- We could also momment with voices we say
--- We could also comment with images, but this need that we have some libraty/Gallery etc. from where we give sensation to the Robot

Robot Item.name: people
  Robot Image (seen->show)  + --------!
  Robot Voice (heard->said) (-)------ ! --!
People log to present <-              !   !
  People like his/her picture --- ----!   !   
  People say Hello                        !
  - microphne button                      !
  People dislike a Voice he heard         !
     associated toa item.name -------------  
- Android-version

- Study Presence by Location -consept
  Maybe we should implement also some location group where this kind action is valid.

- Add ReactionToSensation-attribute to Sensation so if we we have many Robots in the network, then first can one reacts
  in Communication and also in TensorflowClassification to create new Sensations based on source Sensation(s). Other Robots
  must not react when or to some actions to like play a sound if some other Robot has done it already.

- (Main)Robot should be a Sensation or should include sensation(s) that have identity information
  and associations to them , meaning that Robot can feel of things (Sensation)
- (Main)Robot should talk as it's kind. Try that wall-E -kind Robot speeks for
  instance 2/3 rate ( meaning, that we use produce 2/3 sample amount of 1
  
- Robot should deliver sensations only to subRobots running
-- low priority, because problen comes, if some subrobot die and they don't if no developing problems.  
  


What is done
------------

- Architecture, that is
-- devided into Robots that are implemented by Bobots, meaning
--- inheritance
---- Robot can be anywhere in one running instance
--- multihosted, Robots can be anywhere in the world, in any host
-- identity
--- Robot knows some details of itself
-- Virtual instances
--- this is for testing purposes
---- We can create sensations to our Real instance and study how it reacts
     meaning that we can finally start to implement thast our Robots gets Sensations
     that nake to fall in love, our final goal, make a Robot that can feel.
     But before that we have some to do.
-- Static LongTerm Memory
---- All whar Robot knows about this worlds in imlemented as Senmation class
    instances which are connected with each other. Anything other way is used.
    This momocs human memory. 
--- When a rub stoppes, we can save satus of memory and kload it back, when
    new run starts 
-- Item
--- This is a Sensation connected to other Sensation, just a name of a thing
    and a  score for this
-- Tensorflow classification to analyse Images Robot gets. This produces
   Items connected to subImages, cropped from camera Images is Classification
   results. This proces gives to Robot capability so see thing and give a name
   to the seen things and capability to connect other Sensations like voices
   to this Item. This mimics our brain main functionality: we are good at 
   processing visual things and that is most that our brains do. That will be
   main thing Robot will do, just process Images and think what it has seen.
   
- Make clearer git-directory order
-- 1) Wall-E
   Project-name
--- 2)Robot
    - Robot platform
---- 3) linux
     - linux specific files like starting service   
---- 3) python3
----- 4) Robot
      - python3 Robot module implementation
----- 4) package
      - package making scripts
      - package     
---- 3) android
     - android implementation
     

   
What is not done and needs implementation
-----------------------------------------

Robot.process
- divide into two phasess
-- processSensation(sensation)
--- if there is something in Axon, process it
    AlsaPlayback is ecaple of implementation and
    also LoggerRobot explayned below
-- produceSensation()
--- when Sensation are processed, we can sense: as a sense produce a sensation
    default implementation for this is empty, but AlsaMicrophone is an example of implementation,
    as well raspberryPiCamera
- Sensation
-- Sourcerobot

- LoggerRobot
-- interrested of everything
-- Logges current sensations by order
-- whows images seen by order, yhese are diffent we show
-- echoes voices heard, these are different than voices spoken

- LoadSpeakerRobot == AlsaAudioPlayBack, for communicating by Voices with Items
-- TODO howto implement Robot which code is same than other,
   but work and /etc directory is different?
--- We can implement links, same way than linux does
--- or just inherit a robot by import,
    this gives us possibility to separater technology (AlseSpearker) and role (mouth, logger)
- ViewerRobot, for communication by imeges with Items
- We can name of main class of a seen thing. Names are like 'person',
  'dining table', 'bicycle', 'tv' etc., but we cant yet identity things inside
  main class like 'John' is a 'person'. So Robot don't know id it sees 'John' or
  'Joan', it sees just 'person'.
-- implementing this needs
--- live Tesorflow model. We use now foren one. Learning means that we should
    find out characteristic things between 'John' and 'Joan'. These can be voice
    or some statistical thing in images like colors.
- Live Tensorflow model for voices. Image gives a name of seen thing. We connect
  name to voice heard same time thing has been seen. Most potential way to
  implement this kind Ternsorflow learning model for voices is use converting
  scalar voice information as one channel (blachwhite) image information as
  describet in Tensorflow voice analysing totorial pages.
- Expectations.
-- We can study statistically what is happening in hours of day,
  days in week, etc. and make Robot to wait that same kind thins happen and if
  Robot finds out that thing are going on in normal way, make Robot happy. We
  are implementing Feelings. We are near out goal are'nt we?
-- When something happens like we hear a voice identified to Sensorflow to
   a class name, meaning on Item with this name, connected to other kind Sensation
   like Image, we can implement expectation: if something happens, then it
   is expected to happen also something else. Here we can make our Robot happy
   if Robot is right and it has leaned something of thos world. We are even closer
   to our goal, are'nt we?
   
Dependencies
------------  

dependencies: for pillow (pip3 install pillow)
sudo apt-get install libjpeg-dev -y
sudo apt-get install zlib1g-dev -y
sudo apt-get install libfreetype6-dev -y
sudo apt-get install liblcms1-dev -y
sudo apt-get install libopenjp2-7 -y
sudo apt-get install libtiff5 -y

sudo pip3 install pillow


# senses
sudo pip3 install pyalsaaudio
# needs to set AlsaAudioMicrophonw/etc/Robot.cfg
## default:CARD=Device

sudo pip3 install picamera
Camra must be enable from raspi-config

numpy needs
sudo apt-get install python-dev libatlas-base-dev

sudo pip3 install numpy

MainRobot needs
sudo pip3 install python-daemon
sudo pip3 install lockfile
sudo pip3 install psutil

Visual needs
sudo pip3 install wxpython
raspberry pi
- https://wiki.wxpython.org/BuildWxPythonOnRaspberryPi
sudo apt-get install dpkg-dev build-essential libjpeg-dev libtiff-dev libsdl1.2-dev libgstreamer-plugins-base0.10-dev libnotify-dev freeglut3 freeglut3-dev libwebkitgtk-dev libghc-gtk3-dev libwxgtk3.0-gtk3-dev
wget https://files.pythonhosted.org/packages/b9/8b/31267dd6d026a082faed35ec8d97522c0236f2e083bf15aff64d982215e1/wxPython-4.0.7.post2.tar.gz
tar xf wxPython-4.0.7.post2.tar.gz
cd wxPython-4.0.7.post2
sudo pip3 install -r requirements.txt
--- parsberry pi 3b with 1 MB ram could compile if 2 threads
python3 build.py build bdist_wheel --jobs=2
sudo pip3 install dist/wxPython-4.0.7.post2-cp37-cp37m-linux_armv7l.whl

(pip3 install pyyaml # Required to save tensorflow models in YAML format
 not yet implemented ready)

raspberry pi 3b+ Buster Lite
Needs pip3
sudo apt install python3-pip

Tensorflow with Lite, officional package is broken for Lite
python 3.5
https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp35-none-linux_armv7l.whl

python 3.7 (buster)
sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl
gives tensorflow==2.1, which is cuttent when this was written 
(if not work, old instruction
(https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp37-none-linux_armv7l.whl
tensorflow==1.8

Ubuntu 18.4
tensorflow==1.5? -> Last version to work with virtual machine
Ubuntu 14.04
sudo -H pip3 install --upgrade tensorflow --ignore-installed six
-- will get tensorflow 1.13
sudo -H pip3 install --upgrade tensorflow
-- will get tensorflow 1.14
requires
google.protobuf
matplotlib==2.1.0
requires
- sudo apt-get install libfreetype6-dev

OK also you need models from data, application should download these, but the is an error. TODO

Hardware
--------

Study these devices
- Sipeed MAix
-- Support MicroPython on M1
-- out of stock
- HUSKYLENS
-- kickstart,
-- emailed for supoort how to use and for free sample
- Google AI Camera set
- pixy2
-- color signature
-- no tensorflow
- Ai-Thinker ESP32-CAM
-- low
-- face recognition and detection

Develop
------
Done
----

OK Not needed any more
  starting MainRobot with Tensorflow by command line needs now
  PYTHONPATH=/home/reijo/models/research python3 MainRobot.py --start
  and you should copy git/Tensorflow/models/research to that place

OK Seems that capability sharing is not working: Wally does not know that PV can handle Voice In (voices to be spoken)
   Seems that capabilities are transferred OK

OK  We have setting and support for 1-channel microprone now
AlsaAudioMicrophone produces mono, 1-channel file with raspberry USB-microphone
even if setting are 2-channels. Still output is expected to have 2-channels.

OK  Check AlsaAudioMicrophonePlayback logic.
Should read microphone all the time, so we get right average voice level.
Should not be interesting if items are present.
If voice found, then should read as long as voice is on, so we get nice voice.

OK Found fast enoung LITE model and a way to run it for raspberry.
OK TODO check logic and test

- DONE permanent=False -attribute to sensations. This is not set as bytes, but is local property
OK Add not-saveable attribute isForgettable (int)/ reserve(),release() inUse(int) to Senation,
so these Sensation will be reachable as long as Robot.process will process them.
 
OK Start to use LongTerm-memory, when we get an response, so we remember things that should be rembembered.
In Memory-version is ready, but we should study also datavase version.

OK  File "/home/reijo/Wall-E/Robot/Sensation.py", line 1506, in removeAssociation
    del sensation.associations[i]
OK IndexError: list assignment index out of range
 
OK AlsaAudioPlayback: Normalize voices to play
OK Voice Kind cannot be local only, because Communinication and AlsaAudioBroadcast can be in different servers

OK Study memory usage
- import resource
- resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
Deprfecated Change Sensation.create so, that it returns an old Sensation if requested data is close enough.
            This would help also to memory problems, even id we still must implemnt some memory  management.
            Maybe not good idea. Or we should change old  Sensation with new one. But this is not simple to implement.

OK In raspberry application easily runs out of menory
OK -- should implement some method to remove more efficiently not so important sensations

OK def tracePresents(self, sensation):
- to Robot.method
- used in MainRobot
- OK We know present item-names now 

OK Robot.process to support asscociation
-- with that we can ass Feeling to processing (for instance speeking out with a feeling)
-- there is no implementation for this

OK Association to support presence

OK Develop VirtualRobot
- acts as Item.name
- produces Images as Camera does
- Produces Voices as AlsaMicroprone does
- Listens if Robot speaks and answers
- checks if is is marked to be present or not

OK Finalize presense
- Presence works in Tensorflow and AlsaMictophone, associating voices and Images to Item.name present

OK Comminication need to be implented also with supporting presense
- do this test drive
-- support testTensorlow. so real Image and Item sensations are created
-- Voices should be added manually, because AlsaAudioMicrophone can't be test.driven yelt, we can't emulate real devices

OK SocketServer keeps running on error
   SocketServer:('192.168.117.7', 2000):3:Interrupted: run: self.sock.recv(sensation_length) Interrupted error ('192.168.117.7', 2000) [Errno 9] Bad file descriptor


OK Make helper scrits to start and stop MainRobot and write to /tmp/files
   We have servoce-scrip now

OK SocketClient/SocketServer
  Check that it tells only pure Capabilities, not Capabilities from remote Robots
  Other way we get too much traffic. Architecture will be  1:n socket-view
   
OK Tensorflow
  Implement presense so that it report only presense changes
  Timestamp should be same than the original Image timestamp
  
OK Assosiation
  Keep status of present Item.names and assosiate only to present ones, not
  all the stuff.
  Time-based logic is not needed any more
  
OK Sensation
  Report hosts from sensatios is driven
  
 
OK AlsaAudiMicrophonePlayback does not stop. Add stop-handling


OK Implement manualThreading
- very light step that can be run, just sense() to queue or precess() from queue
  -- study how to do this
  -- priority of Robots
  -

OK Created 3 SocketServer and 3 SockertClient threads/instances. 1 +1 is enough.

