Critical
--------

Connections can be duplicated
SocketClient:('192.168.117.7', 2000):3:Normal: got sensation from queue D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.7', 2000):3:Normal: process: Sun Jul 28 22:29:13 2019 D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.5', 2000):3:Normal: got sensation from queue D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.5', 2000):3:Normal: process: Sun Jul 28 22:29:13 2019 D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.7', 51518):3:Normal: got sensation from queue D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.7', 51518):3:Normal: process: Sun Jul 28 22:29:13 2019 D Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketClient:('192.168.117.7', 51518):3:Normal: SocketClient.sendSensationt wrote sensation Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image to ('192.168.117.7', 51518)
SocketServer:('192.168.117.7', 51518):3:Normal: run: waiting size of next Sensation from ('192.168.117.7', 51518)
SocketServer:('192.168.117.7', 51518):3:Normal: run: SocketServer Client ('192.168.117.7', 51518) wrote 15892
SocketServer:('192.168.117.7', 51518):3:Normal: run: SocketServer got sensation Sun Jul 28 22:29:13 2019 1564342228.0706725 Sensory Out Image
SocketServer:('192.168.117.7', 51518):3:Normal: run: waiting next Sensation from('192.168.117.7', 51518)

- Robot should deliver sensations only to subRobots running
- (Main)Robot should be a Sensation or should include sensation(s) that have identity information
  and associations to them , meaning that Robot can feel of things (Sensation)
- (Main)Robot should talk as it's kind. Try that wall-E -kind Robot speeks for
  instance 2/3 rate ( meaning, that we use produde 2/3 sample amount of 1
- DONE permanent=False -attribute to sensations. This is not set as bytes, but is local property

Develop
-------

Study memory usage
- import resource
- resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
Change Sensation.create so, that it returns an old Sensation if requested data is close enough.
This would help also to memory problems, even id we still must implemnt some memory  management.

In raspberry application easyly runs out of menory
-- should implement some method to remove more efficiently not so important sensations

def tracePresents(self, sensation):
- to Robot.method
- used in MainRobot

OK Robot.process to support asscociation
-- with that we can ass Feeling to processing (for instance speeking out with a feeling)
-- there is no implementation for this

Association to support presence

Develop VirtualRobot
- acts as Item.name
- produces Images as Camera does
- Produces Vices as AlsaMicroprone does
- Listens if Robot speaks and answers
- checks if is is marked to be present or not

Finalize presense
- Presence works in Tensorflow and AlsaMictophone, associating voices and Images to Item.name present

Comminication need to be implenteds also supporting presense
- do this test drive
-- support testTensorlow. so real Image and Item sensations are created
-- Voices should be added manually, because AlsaAudioMicrophone can't be test.driven yelt, we can't emulate real devices

SocketServer keeps running on error
SocketServer:('192.168.117.7', 2000):3:Interrupted: run: self.sock.recv(sensation_length) Interrupted error ('192.168.117.7', 2000) [Errno 9] Bad file descriptor


Make helper scrits to start and stop MainRobot and write to /tmp/files

SocketClient/SocketServer
  Check that it tells only pure Capabilities, not Capabilities from remote Robots
  Other way we get too much traffic. Architecture will be  1:n socket-view
   
Tensorflow
  Implement presense so that it report only presense changes
  Timestamp should be same than the original Image timestamp
  
Assosiation
  Keep status of present Item.names and assosiate only to present ones, not
  all the stuff.
  Time-based logic is not needed any more
  
Sensation
  Report hosts from sensatios is driven
  
 
AlsaAudiMicrophonePlayback does not stop. Add stop-handling


Implement manualThreading
- very light step that can be run, just sense() to queue or precess() from queue
  -- study gow to do this
  -- priority of Robots
  -



Created 3 SocketServer and 3 SockertClient threads/instances. 1 +1 is enough.


What is done
------------

- Architecture, that is
-- devided into Robots that are implemented by Bobots, meaning
--- inheritance
---- Robot can be anywhere in one running instance
--- multihosted, Robots can be anywhere in the world, in any host
-- identity
--- Robot knows some details of itself
-- Virtual instances
--- this is for testing purposes
---- We can create sensations to our Real instance and study how it reacts
     meaning that we can finally start to implement thast our Robots gets Sensations
     that nake to fall in love, our final goal, make a Robot that can feel.
     But before that we have some to do.
-- Static LongTerm Memory
---- All whar Robot knows about this worlds in imlemented as Senmation class
    instances which are connected with each other. Anything other way is used.
    This momocs human memory. 
--- When a rub stoppes, we can save satus of memory and kload it back, when
    new run starts 
-- Item
--- This is a Sensation connected to other Sensation, just a name of a thing
    and a  score for this
-- Tensorflow classification to analyse Images Robot gets. This produces
   Items connected to subImages, cropped from camera Images is Classification
   results. This proces gives to Robot capability so see thing and give a name
   to the seen things and capability to connect other Sensations like voices
   to this Item. This mimics our brain main functionality: we are good at 
   processing visual things and that is most that our brains do. That will be
   main thing Robot will do, just process Images and think what it has seen.
   
What is not done and needs implementation
-----------------------------------------

Robot.process
- divide into two phasess
-- processSensation(sensation)
--- if there is something in Axon, process it
    AlsaPlayback is ecaple of implementation and
    also LoggerRobot explayned below
-- produceSensation()
--- when Sensation are processed, we can sense: as a sense produce a sensation
    default implementation for this is empty, but AlsaMicrophone is an example of implementation,
    as well raspberryPiCamera
- Sensation
-- Sourcerobot

- LoggerRobot
-- interrested of everything
-- Logges current sensations by order
-- whows images seen by order, yhese are diffent we show
-- echoes voices heard, these are different than voices spoken

- LoadSpeakerRobot == AlsaAudioPlayBack, for communicating by Voices with Items
-- TODO howto implement Robot which code is same than other,
   but work and /etc directory is different?
--- We can implement links, same way than linux does
--- or just inherit a robot by import,
    this gives us possibility to separater technology (AlseSpearker) and role (mouth, logger)
- ViewerRobot, for communication by imeges with Items
- We can name of main class of a seen thing. Names are like 'person',
  'dining table', 'bicycle', 'tv' etc., but we cant yet identity things inside
  main class like 'John' is a 'person'. So Robot don't know id it sees 'John' or
  'Joan', it sees just 'person'.
-- implementing this needs
--- live Tesorflow model. We use now foren one. Learning means that we should
    find out characteristic things between 'John' and 'Joan'. These can be voice
    or some statistical thing in images like colors.
- Live Tensorflow model for voices. Image gives a name of seen thing. We connect
  name to voice heard same time thing has been seen. Most potential way to
  implement this kind Ternsorflow learning model for voices is use converting
  scalar voice information as one channel (blachwhite) image information as
  describet in Tensorflow voice analysing totorial pages.
- Expectations.
-- We can study statistically what is happening in hours of day,
  days in week, etc. and make Robot to wait that same kind thins happen and if
  Robot finds out that thing are going on in normal way, make Robot happy. We
  are implementing Feelings. We are near out goal are'nt we?
-- When something happens like we hear a voice identified to Sensorflow to
   a class name, meaning on Item with this name, connected to other kind Sensation
   like Image, we can implement expectation: if something happens, then it
   is expected to happen also something else. Here we can make our Robot happy
   if Robot is right and it has leaned something of thos world. We are even closer
   to our goal, are'nt we?
   
Dependencies
------------  

dependencies: for pillow (pip3 install pillow)
sudo apt-get install libjpeg-dev -y
sudo apt-get install zlib1g-dev -y
sudo apt-get install libfreetype6-dev -y
sudo apt-get install liblcms1-dev -y
sudo apt-get install libopenjp2-7 -y
sudo apt-get install libtiff5 -y

MainRobot needs
pip3 install daemon
pip3 install lockfile
pip3 install pyyaml # Required to save tensorflow models in YAML format

raspberry pi 3
tensorflow==1.8
Ubuntu 18.5
tensorflow==1.5
Ubuntu 14.04
sudo -H pip3 install --upgrade tensorflow --ignore-installed six
-- will get tensorflow 1.13
requires
google.protobuf
matplotlib==2.1.0
requires
- sudo apt-get install libfreetype6-dev

starting MainRobor with Tensorflow by command line needs now
PYTHONPATH=/home/reijo/models/research python3 MainRobot.py --start

and you should copy git/Tensorflow/models/research to that place

OK also you need models from data, application should download these, but the is an error. TODO

Hardware
--------

Study
- Sipeed MAix
-- TensorFlow Lite!
-- Support MicroPython on M1
-- out of stock
- HUSKYLENS
-- kickstart
- Google AI Camera set
- pixy2
-- color signature
-- no tensorflow
- Ai-Thinker ESP32-CAM
-- low
-- face recognition and detection
